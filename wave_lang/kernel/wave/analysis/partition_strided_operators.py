# Copyright 2025 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

from copy import deepcopy
from itertools import groupby
from operator import itemgetter

import numpy as np
import sympy
import torch.fx as fx

from wave_lang.support.logging import get_logger

from ..._support.indexing import IndexSequence, IndexSymbol
from ..._support.tracing import CapturedTrace
from ...lang.global_symbols import *
from ...ops.wave_ops import (
    BitcastOp,
    CustomOp,
    ExtractSlice,
    Read,
    Reshape,
    ScaledMMA,
    SelfIndex,
    Write,
    get_custom,
)
from ..constraints import Constraint
from ..utils.tag_utils import propagate_tag
from ..utils.general_utils import (
    all_equal,
    get_fastest_index,
    get_hardware_constraint,
    get_largest_index_and_size,
    infer_dim,
)
from ..utils.mma_utils import (
    simplify_index,
)
from ..utils.symbol_utils import (
    simplify as sym_simplify,
    subs_idxc,
)

logger = get_logger("wave.partition_strided_operators")


def get_vector_shape(
    vector_shapes: dict[IndexSymbol, int],
    symbolic_shape: list[IndexSymbol],
) -> list[int]:
    # Normalize scaled dimensions (like K/2) to base dimensions (like K) for lookup
    result = [max(vector_shapes.get(infer_dim(dim), 1), 1) for dim in symbolic_shape]
    return result


def _get_symbolic_shape_and_vector_shapes(
    custom: CustomOp,
):
    register_shape = custom.register_type.symbolic_shape
    vector_shapes = custom.vector_shapes
    memory_shape = custom.memory_type.symbolic_shape
    # Check to see if the memory shape does not match with the vector shapes.
    if not set(memory_shape).issubset(set(vector_shapes.keys())):
        return register_shape, vector_shapes
    # Pick the shape with the most dimensions.
    if len(memory_shape) > len(register_shape):
        return memory_shape, vector_shapes
    return register_shape, vector_shapes


def partition_strided_operators(trace: CapturedTrace, constraints: list[Constraint]):
    """
    This function analyzes the index sequence of operators in the graph
    that are writes on 2d tensors. If the operator has an access pattern where
    the strides are greater than one on a single dimension, this function splits the
    operands into individual elements and constructs a write for
    each individual element.
    """

    def has_strided_access(node: fx.Node) -> bool:
        """
        Checks for writes on 2d tensors with strided access on a single dimension that
        read more than a single element.
        """
        custom = get_custom(node)
        if isinstance(custom, Write):
            if custom.elements_per_thread == 1:
                return False

            # `custom.register_index` calls are expensive, try to minimize the number
            # of calls.
            strides_and_sizes = [
                (
                    val.stride,
                    val.size,
                )
                for val in map(simplify_index, custom.register_index.values())
            ]
            strides = [x for x, y in strides_and_sizes if y > 1]
            num_strided_accesses = sum(1 for stride in strides if stride > 1)
            if num_strided_accesses > 1:
                raise NotImplementedError(
                    "Support for strided accesses on more than one dimension not implemented yet!"
                )
            return num_strided_accesses == 1
        return False

    strided_operators = trace.walk(has_strided_access)
    for operator in strided_operators:
        custom = get_custom(operator)
        register_index = custom.register_index
        simplified_index = {
            dim: simplify_index(register_index.get(dim, idx))
            for dim, idx in custom.index.items()
        }

        symbolic_shape, vector_shapes = _get_symbolic_shape_and_vector_shapes(custom)

        shape = get_vector_shape(vector_shapes, symbolic_shape)
        elements_per_thread = subs_idxc(custom.elements_per_thread)
        max_stride_dim, max_stride = max(
            [(dim, seq.stride) for dim, seq in simplified_index.items()],
            key=lambda item: item[1],
        )

        # Compute offsets we will aplly to each index element for each partitioned
        # write.
        offsets = np.array(
            [
                np.unravel_index(int(i * max_stride), shape)
                for i in range(elements_per_thread)
            ]
        )

        def check_contiguous_index():
            """
            Check if resulted partitioned write is equivalent to contiguous write.

            Write is contiguous if offsets has form `[0,1,2...N]` for
            the fastest mem dim and 0s for all others dims.
            """
            fastest_mem_dim = custom.memory.type.symbolic_shape[-1]

            # Find fastest mem dim index in the register symbolic shape.
            # If we have write with mapping, order of dims between register and
            # mem may differ or mem dims may not even present in reg index.
            if fastest_mem_dim not in symbolic_shape:
                return False

            fastest_mem_dim_idx = symbolic_shape.index(fastest_mem_dim)

            # Construct expected offsets in the form:
            # [[0 0 0]
            #  [0 1 0]
            #  [0 2 0]
            #  [0 3 0]]
            expected_offsets = np.array(
                [
                    [
                        (j if i == fastest_mem_dim_idx else 0)
                        for j in range(elements_per_thread)
                    ]
                    for i in range(len(shape))
                ]
            ).T
            if not np.array_equal(offsets, expected_offsets):
                return False

            mapping = custom.mapping
            # For writes without mapping this is enough.
            if mapping is None:
                return True

            # If we have mapping, check that fastest dim mapping is trivial, i.e.:
            # IndexMapping(inputs={X: i, ...}, outputs={X: i, ...})
            return (
                mapping.input_mapping.get(fastest_mem_dim, None)
                == mapping.output_mapping[fastest_mem_dim]
            )

        if check_contiguous_index():
            continue

        ops_to_combine = []
        with custom.graph.inserting_before(operator):
            for i in range(elements_per_thread):
                # Non-contiguous access patterns can have varying offsets. We
                # handle that here.
                extract = ExtractSlice(custom.register_, [i], [1], [1]).add_to_graph(
                    custom.graph, loc=custom.location
                )

                offset = offsets[i]
                write = Write(
                    extract,
                    custom.memory,
                    mapping=custom.mapping,
                    elements_per_thread=1,
                    flags=custom.flags,
                ).add_to_graph(custom.graph, loc=custom.location)
                write.index = {
                    dim: IndexSequence(
                        simplified_index[dim].start.subs({GPR_NUM: 0}) + offset[j], 1, 1
                    )
                    for j, dim in enumerate(symbolic_shape)
                }
                extract.index = write.index
                write.vector_shapes = vector_shapes
                ops_to_combine.append(write)

        # Useful to handle write/read dependency
        custom.replace_all_uses_with(ops_to_combine)
        custom.graph.erase_node(operator)


def partition_ops_with_gpr_offsets(trace: CapturedTrace, constraints: list[Constraint]):
    """
    This function analyzes the index sequence of reads and writes in a graph.
    If the reads or writes have incontiguous offsets based on GPR_NUM, we'd
    need to split these reads/writes appropriately.

    e.g a vector<16xf16> may be owned by lane 0, and lane 16 in this layout:
    [0, 0, 0, 0, 16, 16, 16, 16, 0, 0, 0, 0, 16, 16, 16, 16].

    With our current glossary, this means we have 2 VGPR "chunks".
    [0:4) and [8:12) for lane0, and [4:8) and [12:16) for lane16.
    To the lane it should just look like vector<8xf16>.
    Hence for this example, we'd need two reads of vector<4xf16> and a couple
    insert_slices to combine them together into a single vector<8xf16>.

    """

    def has_gpr_offsets(node: fx.Node) -> bool:
        """
        Checks for writes on 2d tensors with strided access on a single dimension that
        read more than a single element.
        """
        custom = get_custom(node)
        if not isinstance(custom, (Read, Write, SelfIndex)):
            return False
        num_dims_with_gpr = sum(
            1 for v in custom.index.values() if sympy.sympify(v.start).has(GPR_NUM)
        )
        if num_dims_with_gpr == 1:
            return True
        elif num_dims_with_gpr == 0:
            return False
        raise NotImplementedError("Currently only handles 1 dim with GPR offset.")

    strided_operators = trace.walk(has_gpr_offsets)
    for operator in strided_operators:
        custom = get_custom(operator)
        index = custom.index
        simplified_index = {
            dim: simplify_index(index.get(dim, idx)) for dim, idx in index.items()
        }
        if isinstance(custom, SelfIndex):
            # If specified use element_per_thread instead of IndexExpr size.
            elements_per_thread = subs_idxc(
                custom.elements_per_thread or index[custom.dim].size
            )
        else:
            elements_per_thread = subs_idxc(custom.elements_per_thread)
        dim_with_gpr_offsets = [
            (k, v.start) for k, v in simplified_index.items() if v.start.has(GPR_NUM)
        ]
        assert len(dim_with_gpr_offsets) == 1, "Expected only 1-Dim has gpr offsets"
        gpr_offset_dim, gpr_offset_expr = dim_with_gpr_offsets[0]
        gpr_offsets = [
            gpr_offset_expr.subs({GPR_NUM: i}) for i in range(elements_per_thread)
        ]

        # Cluster contiguous indices of reads/writes
        # e.g given indices  [0, 1, 2, 3, 8, 9, 10, 11, 16, 17, 18, 19, 24, 25, 26, 27]
        # Will cluster to:  [[0, 1, 2, 3], [8, 9, 10, 11], [16, 17, 18, 19], [24, 25, 26, 27]]
        gpr_relative_offsets = [x - gpr_offsets[0] for x in gpr_offsets]
        gpr_chunks = [
            list(map(itemgetter(1), g))
            for _, g in groupby(enumerate(gpr_relative_offsets), lambda x: x[0] - x[1])
        ]

        # Compute number of GPR chunks.
        num_gpr_chunks = len(gpr_chunks)

        # Compute size of each chunk and ensure they are the same size.
        gpr_sizes = [len(gpr_chunk) for gpr_chunk in gpr_chunks]
        assert all_equal(
            gpr_sizes
        ), "Only support strided GPR offset with uniform sizes."
        gpr_size = gpr_sizes[0]

        # Break apart Reads/Writes that has non-contiguous GPR Read/Writes.
        with custom.graph.inserting_before(operator):
            ops_to_combine = []
            for chunk_id in range(num_gpr_chunks):
                cur_gpr_start_id = chunk_id * gpr_size
                # Get updated index with VGPR offset.
                if hasattr(custom, "mapping") and custom.mapping is not None:
                    output_mapping = list(custom.mapping.output_mapping.keys())
                else:
                    output_mapping = list(index)
                # Modify stride to 1 S.T we can have vectorized read/write
                # iff gpr_offset_dim is or will be (after mapping) fastest dim.
                updated_index_with_gpr_offset = deepcopy(simplified_index)
                updated_dim_with_gpr_offset = IndexSequence(
                    updated_index_with_gpr_offset[gpr_offset_dim].start.subs(
                        {GPR_NUM: cur_gpr_start_id}
                    ),
                    gpr_size,
                    (
                        1
                        if output_mapping[-1] == gpr_offset_dim
                        else simplified_index[gpr_offset_dim].stride
                    ),
                )
                updated_index_with_gpr_offset[gpr_offset_dim] = (
                    updated_dim_with_gpr_offset
                )

                if hasattr(custom, "mapping_dynamic_vals"):
                    # If we are partitioning read/write ops, dynamic_vals can be
                    # potentially partitioned as well. Partitioned dyn vals are
                    # are merged into single value using Reshape op which still
                    # holds the original index containing `GPR_NUM`.
                    # Extract corresponding partitioned chunk from such ops.
                    new_dynamic_vals = []
                    for dyn_val in custom.mapping_dynamic_vals:
                        if any(
                            sympy.sympify(v.start).has(GPR_NUM)
                            for v in get_custom(dyn_val).index.values()
                        ):
                            extract = ExtractSlice(
                                dyn_val, [cur_gpr_start_id], [gpr_size], [1]
                            ).add_to_graph(custom.graph, loc=custom.location)
                            extract.index = updated_index_with_gpr_offset
                            new_dynamic_vals.append(extract)
                        else:
                            new_dynamic_vals.append(dyn_val)

                # Generate new Read/Write that has contiguous VGPR elements.
                if isinstance(custom, Write):
                    extract = ExtractSlice(
                        custom.register_, [cur_gpr_start_id], [gpr_size], [1]
                    ).add_to_graph(custom.graph, loc=custom.location)
                    extract.index = updated_index_with_gpr_offset
                    new_node = Write(
                        extract,
                        custom.memory,
                        mapping=custom.mapping,
                        mapping_dynamic_vals=new_dynamic_vals,
                        elements_per_thread=gpr_size,
                        flags=custom.flags,
                    ).add_to_graph(custom.graph, loc=custom.location)
                elif isinstance(custom, Read):
                    # TODO: Add support on how to handle strided reads.
                    new_node = Read(
                        custom.memory,
                        elements_per_thread=gpr_size,
                        mapping=custom.mapping,
                        mapping_dynamic_vals=new_dynamic_vals,
                        _write_dependency=custom._write_dependency,
                        flags=custom.flags,
                    ).add_to_graph(custom.graph, loc=custom.location)
                elif isinstance(custom, SelfIndex):
                    # iff elements_per_thread is specified, we update
                    # elements_per_thread to chunk size, else return None.
                    self_index_size = gpr_size if custom.elements_per_thread else None
                    new_node = SelfIndex(
                        custom.dim, custom.dtype, self_index_size
                    ).add_to_graph(custom.graph, loc=custom.location)

                # Update new_node information
                new_node.index = updated_index_with_gpr_offset
                new_node.vector_shapes = custom.vector_shapes
                ops_to_combine.append(new_node)

            # Update users of original op.
            if isinstance(custom, Write):
                # Useful to handle write/read dependency
                custom.replace_all_uses_with(ops_to_combine)
            elif isinstance(custom, (Read, SelfIndex)):
                reshape = Reshape(ops_to_combine, custom.vector_shapes).add_to_graph(
                    custom.graph, loc=custom.location
                )
                reshape.expanded_dims = custom.expanded_dims
                reshape.vector_shapes = custom.vector_shapes
                propagate_tag(custom.fx_node, reshape)
                # Also propagate tag to the underlying Read nodes
                for op in ops_to_combine:
                    propagate_tag(custom.fx_node, op)

                # Save the original index on the reshape op so later we can
                # detect if op was part of `gpr_offset` partition.
                reshape.index = index
                custom.replace_all_uses_with(reshape)

            custom.graph.erase_node(custom.fx_node)


def merge_contiguous_reads(
    trace: CapturedTrace, constraints: list[Constraint], target: str
):
    """
    Merge reads that access contiguous physical memory into wider vector loads.

    Runs to a fixed point, doubling the vector width each iteration:
    ept=1 pairs → ept=2, ept=2 pairs → ept=4, etc. Works regardless of how
    the reads were created (expansion, manual, etc.).

    Reads are grouped by (memory operand, ept). Within each group, pairs whose
    physical flat offset starts differ by exactly ept are merged.
    """
    hw_constraint = get_hardware_constraint(constraints)
    while _merge_contiguous_reads_once(trace, hw_constraint):
        pass


def _get_physical_start(
    custom: Read,
    symbolic_shape: tuple,
    symbolic_dims: list,
) -> dict:
    """Get the physical start coordinates for a read.

    For reads with a non-identity mapping, applies the mapping to get physical
    coordinates. For identity-mapped reads (mapping=None), reads the start
    offsets directly from the index.
    """
    from ..utils.mapping_utils import transform_index_on_mapping

    if custom.mapping is not None and not custom.has_identity_mapping():
        physical = transform_index_on_mapping(
            custom.mapping, symbolic_shape, custom.index, is_read=True
        )
        return {dim: physical[dim] for dim in symbolic_dims}
    return {dim: custom.index[dim].start for dim in symbolic_dims}


def _hw_simplified_diff(raw_diff, ept):
    """Fallback contiguity check: strip non-lane symbols and re-simplify.

    For two reads from the same thread/wave/workgroup/iteration, all
    symbols except the lane id ($T0 = threadIdx.x) contribute identical
    offsets and cancel in the diff.  Substituting them to 0 isolates the
    lane-dependent part, which the bounds-based simplifier can resolve.
    """
    free = raw_diff.free_symbols
    if not free:
        return None
    _LANE_SYM = "$T0"
    lane_sym = None
    subs = {}
    for s in free:
        if s.name == _LANE_SYM:
            lane_sym = s
        else:
            subs[s] = 0
    if lane_sym is None:
        return None
    expr = raw_diff.subs(subs)
    if expr.free_symbols - {lane_sym}:
        return None
    reduced = sym_simplify(expr)
    if reduced == ept:
        return 1
    if reduced == -ept:
        return -1
    return None


def _merge_contiguous_reads_once(trace: CapturedTrace, hw_constraint) -> bool:
    """Single merge pass: merge adjacent pairs of same-ept reads.

    Groups reads by (memory operand, ept) and merges pairs whose physical
    flat offset starts differ by exactly ept. Returns True if any merges
    happened.
    """
    from collections import defaultdict
    from ...compiler.utils import strides_from_symbolic_shape
    from ..._support.indexing import IndexingContext

    # Group reads by (memory, ept, region).  A new region starts at each
    # subgraph boundary and whenever a side-effecting op (write, barrier, ...)
    # is encountered, so we never merge reads across such ops.  Reads with
    # dynamic mapping values are skipped to keep the merge logic simple.
    groups: dict[tuple, list[fx.Node]] = defaultdict(list)
    region_id = 0
    for subgraph in trace.region_graph.subgraphs.values():
        region_id += 1
        for node in subgraph.nodes:
            custom = get_custom(node)
            if not isinstance(custom, CustomOp):
                continue
            if custom.has_side_effects:
                region_id += 1
                continue
            if not isinstance(custom, Read):
                continue
            if custom.mapping_dynamic_vals:
                continue
            # Skip reads that have bounds: the merged read would lose the
            # mapping and source→target index, making mask generation incorrect.
            if custom.bounds is not None:
                continue
            key = (custom.memory, custom.elements_per_thread, region_id)
            groups[key].append(node)

    idxc = IndexingContext.current()
    merged_any = False

    for (memory_node, ept, _region), reads in groups.items():
        if len(reads) < 2:
            continue

        customs = [(get_custom(n), n) for n in reads]
        memory = get_custom(memory_node)
        symbolic_shape = memory.type.symbolic_shape
        strides = strides_from_symbolic_shape(
            idxc, symbolic_shape, allow_mixed_shapes=True
        )
        symbolic_dims = [infer_dim(d) for d in symbolic_shape]

        read_infos = []
        for custom, node in customs:
            phys_start = _get_physical_start(custom, symbolic_shape, symbolic_dims)
            flat_offset = sum(
                phys_start[dim] * stride for dim, stride in zip(symbolic_dims, strides)
            )
            read_infos.append((flat_offset, phys_start, custom, node))

        # Try all pairs to find contiguous ones (diff == ept).
        merged = set()
        for i in range(len(read_infos)):
            if i in merged:
                continue
            for j in range(i + 1, len(read_infos)):
                if j in merged:
                    continue
                off1, phys1, custom1, node1 = read_infos[i]
                off2, phys2, custom2, node2 = read_infos[j]

                # Check both orderings: i before j and j before i.
                raw_diff = off2 - off1
                diff = sym_simplify(raw_diff)
                if diff == ept:
                    lo_phys, hi_phys = phys1, phys2
                    lo_custom, hi_custom = custom1, custom2
                    lo_node, hi_node = node1, node2
                elif diff == -ept:
                    lo_phys, hi_phys = phys2, phys1
                    lo_custom, hi_custom = custom2, custom1
                    lo_node, hi_node = node2, node1
                else:
                    direction = _hw_simplified_diff(raw_diff, ept)
                    if direction is None:
                        continue
                    if direction == 1:
                        lo_phys, hi_phys = phys1, phys2
                        lo_custom, hi_custom = custom1, custom2
                        lo_node, hi_node = node1, node2
                    else:
                        lo_phys, hi_phys = phys2, phys1
                        lo_custom, hi_custom = custom2, custom1
                        lo_node, hi_node = node2, node1

                # Find dimension that advances by ept.
                merge_dim = None
                for dim in symbolic_dims:
                    raw_d = hi_phys[dim] - lo_phys[dim]
                    d = sym_simplify(raw_d)
                    if d == ept:
                        merge_dim = dim
                    elif not (d == 0):
                        merge_dim = None
                        break
                if merge_dim is None:
                    # Fallback: if flat diff proves adjacency, infer the
                    # stride-1 dimension as merge_dim.  Safe when ept is
                    # smaller than every non-unit stride (no row crossing).
                    unit_dim = None
                    min_other_stride = float("inf")
                    for dim, stride in zip(symbolic_dims, strides):
                        if stride == 1:
                            unit_dim = dim
                        elif isinstance(stride, int):
                            min_other_stride = min(min_other_stride, stride)
                    if unit_dim is not None and ept < min_other_stride:
                        merge_dim = unit_dim
                    else:
                        continue

                # Respect hardware vector width limit.
                new_ept = 2 * ept
                element_type = lo_custom.type.dtype
                if new_ept > hw_constraint.max_elems_per_load(element_type):
                    continue
                with lo_custom.graph.inserting_before(lo_node):
                    new_index = {
                        dim: IndexSequence(
                            lo_phys[dim],
                            new_ept if dim == merge_dim else 1,
                            1,
                        )
                        for dim in symbolic_dims
                    }

                    merged_read = Read(
                        lo_custom.memory,
                        elements_per_thread=new_ept,
                        mapping=None,
                        _write_dependency=lo_custom._write_dependency,
                        flags=lo_custom.flags,
                    ).add_to_graph(lo_custom.graph, loc=lo_custom.location)
                    merged_custom = get_custom(merged_read)
                    merged_custom.index = new_index
                    merged_custom.vector_shapes = deepcopy(lo_custom.vector_shapes)
                    propagate_tag(lo_node, merged_read)

                    extract0 = ExtractSlice(merged_read, [0], [ept], [1]).add_to_graph(
                        lo_custom.graph, loc=lo_custom.location
                    )
                    get_custom(extract0).index = deepcopy(lo_custom.index)
                    get_custom(extract0).vector_shapes = deepcopy(
                        lo_custom.vector_shapes
                    )
                    propagate_tag(lo_node, extract0)

                    extract1 = ExtractSlice(
                        merged_read, [ept], [ept], [1]
                    ).add_to_graph(lo_custom.graph, loc=lo_custom.location)
                    get_custom(extract1).index = deepcopy(hi_custom.index)
                    get_custom(extract1).vector_shapes = deepcopy(
                        hi_custom.vector_shapes
                    )
                    propagate_tag(hi_node, extract1)

                lo_custom.replace_all_uses_with(extract0)
                hi_custom.replace_all_uses_with(extract1)
                lo_custom.graph.erase_node(lo_node)
                hi_custom.graph.erase_node(hi_node)

                merged.update({i, j})
                merged_any = True
                break

    return merged_any


def partition_gather_like_ops(
    trace: CapturedTrace, constraints: list[Constraint], target: str
):
    """
    This pass partitions gather-like operations (reads/writes with non-contiguous access patterns) into
    multiple contiguous operations.

    For example, if we have a write operation that writes elements with stride 2:
    write([0,2,4,6]) -> write([0]), write([2]), write([4]), write([6])

    The pass:
    1. Identifies reads/writes with non-contiguous access patterns
    2. For each such operation:
       - Creates multiple single-element reads/writes
       - Updates indices to access the correct elements
       - Handles dynamic values in mappings
       - Combines results back together if needed (for reads)
    """

    def has_gather_mapping(node: fx.Node) -> bool:
        """
        Checks for writes on 2d tensors with strided access on a single dimension that
        read more than a single element.
        """
        custom = get_custom(node)
        if isinstance(custom, (Read, Write)):
            return not custom.is_contiguous_vec(constraints, target)

        return False

    def _feeds_scaled_mma_scale(node: fx.Node) -> bool:
        """Check if a Read feeds a ScaledMMA scale operand (through BitcastOp).

        Scale reads should keep their vector width (ept=4) so they flow as
        vector<4xf8E8M0FNU> to amdgpu.scaled_mfma with opsel, matching the
        AITER buffer_load_dword pattern.
        """
        for user in node.users:
            user_custom = get_custom(user)
            if isinstance(user_custom, BitcastOp):
                for bitcast_user in user.users:
                    bc_user = get_custom(bitcast_user)
                    if isinstance(bc_user, ScaledMMA):
                        if bc_user.lhs_scale == user or bc_user.rhs_scale == user:
                            return True
            elif isinstance(user_custom, ScaledMMA):
                if user_custom.lhs_scale == node or user_custom.rhs_scale == node:
                    return True
        return False

    strided_operators = trace.walk(has_gather_mapping)
    for operator in strided_operators:
        custom = get_custom(operator)

        if isinstance(custom, Read) and _feeds_scaled_mma_scale(operator):
            continue

        index = custom.index
        fastest_index = get_fastest_index(index)
        elements_per_thread = subs_idxc(custom.elements_per_thread)

        # Break apart Reads/Writes that has non-contiguous access patterns.
        with custom.graph.inserting_before(operator):
            ops_to_combine = []
            for i in range(elements_per_thread):
                new_index = deepcopy(index)
                for j, v in enumerate(new_index.values()):
                    if j == fastest_index:
                        v.start = v.start + i
                    v.size = 1
                    v.stride = 1

                new_dynamic_vals = []
                for dynamic_val in custom.mapping_dynamic_vals:
                    _, size = get_largest_index_and_size(dynamic_val.index)
                    if size == 1:
                        # If size is 1, it means we are broadcasting same dynamic value to all
                        # vector elements.
                        new_dynamic_vals.append(dynamic_val)
                        continue

                    assert (
                        size == elements_per_thread
                    ), f"Expected size to be equal to {elements_per_thread}, got {size}"

                    # Otherwise, we need to extract the dynamic value for the current vector element.
                    extract = ExtractSlice(dynamic_val, [i], [1], [1]).add_to_graph(
                        custom.graph, loc=custom.location
                    )
                    extract.index = new_index
                    new_dynamic_vals.append(extract)

                if isinstance(custom, Write):
                    extract = ExtractSlice(
                        custom.register_, [i], [1], [1]
                    ).add_to_graph(custom.graph, loc=custom.location)
                    extract.index = new_index
                    new_node = Write(
                        extract,
                        custom.memory,
                        mapping=custom.mapping,
                        mapping_dynamic_vals=new_dynamic_vals,
                        elements_per_thread=1,
                        flags=custom.flags,
                    ).add_to_graph(custom.graph, loc=custom.location)
                elif isinstance(custom, Read):
                    new_node = Read(
                        custom.memory,
                        elements_per_thread=1,
                        mapping=custom.mapping,
                        mapping_dynamic_vals=new_dynamic_vals,
                        _write_dependency=custom._write_dependency,
                        flags=custom.flags,
                    ).add_to_graph(custom.graph, loc=custom.location)
                else:
                    raise NotImplementedError(f"Unsupported op type: {custom}")

                # Update new_node information
                new_node.index = new_index
                new_node.vector_shapes = custom.vector_shapes
                ops_to_combine.append(new_node)

            # Update users of original op.
            if isinstance(custom, Write):
                # Useful to handle write/read dependency
                custom.replace_all_uses_with(ops_to_combine)
            elif isinstance(custom, Read):
                reshape = Reshape(ops_to_combine, custom.vector_shapes).add_to_graph(
                    custom.graph
                )
                reshape.expanded_dims = custom.expanded_dims
                reshape.vector_shapes = custom.vector_shapes
                propagate_tag(custom.fx_node, reshape)
                # Also propagate tag to the underlying Read nodes
                for op in ops_to_combine:
                    propagate_tag(custom.fx_node, op)

                reshape.index = index
                custom.replace_all_uses_with(reshape)
            else:
                raise NotImplementedError(f"Unsupported op type: {custom}")

            custom.erase()
