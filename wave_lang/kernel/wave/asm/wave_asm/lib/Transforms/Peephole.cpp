// Copyright 2025 The Wave Authors
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

//===----------------------------------------------------------------------===//
// Peephole Optimization Pass
//
// This pass implements peephole optimizations for WAVEASM IR:
// 1. Instruction fusion: lshl + add -> v_mad_u32_u24
// 2. Dead code elimination
// 3. Redundant move elimination
//===----------------------------------------------------------------------===//

#include "waveasm/Dialect/WaveASMDialect.h"
#include "waveasm/Dialect/WaveASMOps.h"
#include "waveasm/Dialect/WaveASMTypes.h"
#include "waveasm/Transforms/Passes.h"
#include "waveasm/Transforms/Utils.h"

#include "mlir/IR/Builders.h"
#include "mlir/IR/BuiltinOps.h"
#include "mlir/IR/PatternMatch.h"
#include "mlir/Pass/Pass.h"
#include "mlir/Transforms/GreedyPatternRewriteDriver.h"
#include "llvm/ADT/SmallVector.h"
#include "llvm/Support/MathExtras.h"

#include <limits>

using namespace mlir;
using namespace waveasm;

namespace {

//===----------------------------------------------------------------------===//
// Pattern: lshlrev + or -> v_lshl_or_b32
//
// Fuses:
//   %shifted = v_lshlrev_b32 %shift_amt, %src
//   %result = v_or_b32 %shifted, %orend
// Into:
//   %result = v_lshl_or_b32 %src, %shift_amt, %orend
//
// This is a common pattern in address calculations where bit ranges
// don't overlap, allowing OR to be used instead of ADD.
//===----------------------------------------------------------------------===//

struct LshlOrFusePattern : public OpRewritePattern<V_OR_B32> {
  using OpRewritePattern<V_OR_B32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_OR_B32 orOp,
                                PatternRewriter &rewriter) const override {
    // Check if either operand is the result of a left shift
    auto checkOperand = [&](Value shifted, Value orend) -> LogicalResult {
      auto lshlOp = shifted.getDefiningOp<V_LSHLREV_B32>();
      if (!lshlOp)
        return failure();

      // Check that the shift has only one use (this or)
      if (!lshlOp.getResult().hasOneUse())
        return failure();

      // Get the shift amount - must be a constant for efficiency
      auto constOp = lshlOp.getSrc0().getDefiningOp<ConstantOp>();
      if (!constOp)
        return failure();

      // v_lshl_or_b32 operands: src (value to shift), shift_amt, orend
      // In v_lshlrev_b32: src0 = shift_amt, src1 = value to shift
      Value shiftSrc = lshlOp.getSrc1(); // value being shifted
      Value shiftAmt = lshlOp.getSrc0(); // shift amount

      auto loc = orOp.getLoc();
      auto resultType = orOp.getResult().getType();

      // Create v_lshl_or_b32: dst = (src << shift_amt) | orend
      auto fusedOp = V_LSHL_OR_B32::create(rewriter, loc, resultType, shiftSrc,
                                           shiftAmt, orend);

      rewriter.replaceOp(orOp, fusedOp.getResult());
      return success();
    };

    // Try both operand orderings
    if (succeeded(checkOperand(orOp.getSrc0(), orOp.getSrc1())))
      return success();
    if (succeeded(checkOperand(orOp.getSrc1(), orOp.getSrc0())))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: lshl + add -> v_lshl_add_u32
//
// Fuses:
//   %shifted = v_lshlrev_b32 %shift_amt, %base
//   %result = v_add_u32 %shifted, %addend
// Into:
//   %result = v_lshl_add_u32 %base, %shift_amt, %addend
//
// This matches Python's use of the fused shift-add instruction.
//===----------------------------------------------------------------------===//

struct LshlAddPattern : public OpRewritePattern<V_ADD_U32> {
  using OpRewritePattern<V_ADD_U32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_ADD_U32 addOp,
                                PatternRewriter &rewriter) const override {
    // Check if either operand is the result of a left shift
    auto checkOperand = [&](Value shifted, Value other) -> LogicalResult {
      auto lshlOp = shifted.getDefiningOp<V_LSHLREV_B32>();
      if (!lshlOp)
        return failure();

      // Check that the shift has only one use (this add)
      if (!lshlOp.getResult().hasOneUse())
        return failure();

      // Get the shift amount - must be a constant for v_lshl_add_u32
      auto constOp = lshlOp.getSrc0().getDefiningOp<ConstantOp>();
      if (!constOp)
        return failure();

      int64_t shiftAmount = constOp.getValue();

      // Shift amount must be non-negative and <= 31 for 32-bit operations
      if (shiftAmount < 0 || shiftAmount > 31)
        return failure();

      auto loc = addOp.getLoc();

      // Get the base value being shifted
      Value base = lshlOp.getSrc1();

      // Get or create the shift amount constant
      Value shiftConst = lshlOp.getSrc0();

      // Create v_lshl_add_u32: dst = (base << shift) + other
      auto resultType = addOp.getResult().getType();
      auto lshlAddOp = V_LSHL_ADD_U32::create(rewriter, loc, resultType, base,
                                              shiftConst, other);

      rewriter.replaceOp(addOp, lshlAddOp.getResult());
      return success();
    };

    // Try both operand orderings
    if (succeeded(checkOperand(addOp.getSrc0(), addOp.getSrc1())))
      return success();
    if (succeeded(checkOperand(addOp.getSrc1(), addOp.getSrc0())))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Redundant move elimination
//
// Removes:
//   %dst = v_mov_b32 %src
// When %src is already in a VGPR and can be used directly.
//===----------------------------------------------------------------------===//

struct RedundantMovePattern : public OpRewritePattern<V_MOV_B32> {
  using OpRewritePattern<V_MOV_B32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_MOV_B32 movOp,
                                PatternRewriter &rewriter) const override {
    Value src = movOp.getSrc();

    // Skip if source is an immediate (move is necessary)
    if (isa<ImmType>(src.getType()))
      return failure();

    // Skip if source is an SGPR (move to VGPR is necessary for broadcast)
    if (isa<SRegType, PSRegType>(src.getType()))
      return failure();

    // Source is already a VGPR, so the move is redundant
    rewriter.replaceOp(movOp, src);
    return success();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Multiply by power of 2 -> shift
//
// Transforms:
//   %dst = v_mul_lo_u32 %src, %const  (where const is power of 2)
// Into:
//   %dst = v_lshlrev_b32 log2(const), %src
//===----------------------------------------------------------------------===//

struct MulPow2ToShiftPattern : public OpRewritePattern<V_MUL_LO_U32> {
  using OpRewritePattern<V_MUL_LO_U32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_MUL_LO_U32 mulOp,
                                PatternRewriter &rewriter) const override {
    auto checkOperand = [&](Value constVal, Value other) -> LogicalResult {
      auto valueOpt = getConstantValue(constVal);
      if (!valueOpt)
        return failure();

      int64_t value = *valueOpt;

      // Check if value is a positive power of 2
      if (value <= 0 || (value & (value - 1)) != 0)
        return failure();

      int64_t shiftAmount =
          static_cast<int64_t>(llvm::Log2_64(static_cast<uint64_t>(value)));

      // Create the shift constant
      auto loc = mulOp.getLoc();
      auto immType = rewriter.getType<ImmType>(shiftAmount);
      auto shiftConst = ConstantOp::create(rewriter, loc, immType, shiftAmount);

      // Create lshlrev: dst = other << shiftAmount
      auto resultType = mulOp.getResult().getType();
      auto lshlOp =
          V_LSHLREV_B32::create(rewriter, loc, resultType, shiftConst, other);

      rewriter.replaceOp(mulOp, lshlOp.getResult());
      return success();
    };

    // Try both operand orderings
    if (succeeded(checkOperand(mulOp.getSrc0(), mulOp.getSrc1())))
      return success();
    if (succeeded(checkOperand(mulOp.getSrc1(), mulOp.getSrc0())))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Multiply by negative power of 2 -> shift + negate
//
// Transforms:
//   %dst = v_mul_lo_u32 %src, %const  (where const is negative power of 2)
// Into:
//   %shifted = v_lshlrev_b32 log2(-const), %src
//   %dst = v_sub_u32 0, %shifted
//
// This is common in index calculations where negative strides appear.
//===----------------------------------------------------------------------===//

struct MulNegPow2ToShiftNegPattern : public OpRewritePattern<V_MUL_LO_U32> {
  using OpRewritePattern<V_MUL_LO_U32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_MUL_LO_U32 mulOp,
                                PatternRewriter &rewriter) const override {
    auto checkOperand = [&](Value constVal, Value other) -> LogicalResult {
      auto valueOpt = getConstantValue(constVal);
      if (!valueOpt)
        return failure();

      int64_t value = *valueOpt;

      // Check if value is a negative power of 2
      if (value >= 0)
        return failure();

      // Guard against INT64_MIN (negation would overflow)
      if (value == std::numeric_limits<int64_t>::min())
        return failure();

      int64_t absValue = -value;
      if ((absValue & (absValue - 1)) != 0)
        return failure(); // Not a power of 2

      int64_t shiftAmount =
          static_cast<int64_t>(llvm::Log2_64(static_cast<uint64_t>(absValue)));

      auto loc = mulOp.getLoc();

      // Create shift constant
      auto immType = rewriter.getType<ImmType>(shiftAmount);
      auto shiftConst = ConstantOp::create(rewriter, loc, immType, shiftAmount);

      // Create zero constant for negation
      auto zeroImm = rewriter.getType<ImmType>(0);
      auto zeroConst = ConstantOp::create(rewriter, loc, zeroImm, 0);

      // Shift the value: shifted = src << log2(abs(const))
      auto resultType = mulOp.getResult().getType();
      auto lshlOp =
          V_LSHLREV_B32::create(rewriter, loc, resultType, shiftConst, other);

      // Negate the result: dst = 0 - shifted
      auto subOp =
          V_SUB_U32::create(rewriter, loc, resultType, zeroConst, lshlOp);

      rewriter.replaceOp(mulOp, subOp.getResult());
      return success();
    };

    // Try both operand orderings
    if (succeeded(checkOperand(mulOp.getSrc0(), mulOp.getSrc1())))
      return success();
    if (succeeded(checkOperand(mulOp.getSrc1(), mulOp.getSrc0())))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Add zero elimination
//
// Removes:
//   %dst = v_add_u32 %src, 0
// Replacing with %src directly.
//===----------------------------------------------------------------------===//

struct AddZeroPattern : public OpRewritePattern<V_ADD_U32> {
  using OpRewritePattern<V_ADD_U32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_ADD_U32 addOp,
                                PatternRewriter &rewriter) const override {
    auto checkOperand = [&](Value constVal, Value other) -> LogicalResult {
      auto constOp = constVal.getDefiningOp<ConstantOp>();
      if (!constOp)
        return failure();

      if (constOp.getValue() != 0)
        return failure();

      rewriter.replaceOp(addOp, other);
      return success();
    };

    if (succeeded(checkOperand(addOp.getSrc0(), addOp.getSrc1())))
      return success();
    if (succeeded(checkOperand(addOp.getSrc1(), addOp.getSrc0())))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Multiply by one elimination
//
// Removes:
//   %dst = v_mul_lo_u32 %src, 1
// Replacing with %src directly.
//===----------------------------------------------------------------------===//

struct MulOnePattern : public OpRewritePattern<V_MUL_LO_U32> {
  using OpRewritePattern<V_MUL_LO_U32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_MUL_LO_U32 mulOp,
                                PatternRewriter &rewriter) const override {
    auto checkOperand = [&](Value constVal, Value other) -> LogicalResult {
      auto constOp = constVal.getDefiningOp<ConstantOp>();
      if (!constOp)
        return failure();

      if (constOp.getValue() != 1)
        return failure();

      rewriter.replaceOp(mulOp, other);
      return success();
    };

    if (succeeded(checkOperand(mulOp.getSrc0(), mulOp.getSrc1())))
      return success();
    if (succeeded(checkOperand(mulOp.getSrc1(), mulOp.getSrc0())))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Multiply by zero -> constant zero
//
// Transforms:
//   %dst = v_mul_lo_u32 %src, 0
// Into:
//   %dst = constant 0
//===----------------------------------------------------------------------===//

struct MulZeroPattern : public OpRewritePattern<V_MUL_LO_U32> {
  using OpRewritePattern<V_MUL_LO_U32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_MUL_LO_U32 mulOp,
                                PatternRewriter &rewriter) const override {
    auto checkOperand = [&](Value constVal) -> bool {
      auto constOp = constVal.getDefiningOp<ConstantOp>();
      return constOp && constOp.getValue() == 0;
    };

    if (checkOperand(mulOp.getSrc0()) || checkOperand(mulOp.getSrc1())) {
      auto loc = mulOp.getLoc();
      auto immType = rewriter.getType<ImmType>(0);
      auto zeroConst = ConstantOp::create(rewriter, loc, immType, 0);
      // Move to VGPR if needed
      auto vregType = rewriter.getType<VRegType>(1, 1);
      auto movOp = V_MOV_B32::create(rewriter, loc, vregType, zeroConst);
      rewriter.replaceOp(mulOp, movOp.getResult());
      return success();
    }

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: (x >> N) << N -> x & ~((1 << N) - 1)
//
// The affine handler compiles floor(x / 2^N) * 2^N as:
//   %shr = v_lshrrev_b32 N, x
//   %shl = v_lshlrev_b32 N, %shr
// This is equivalent to masking off the low N bits:
//   %result = v_and_b32 x, ~((1 << N) - 1)
// Saves 1 VALU instruction per occurrence.
//===----------------------------------------------------------------------===//

struct ShiftRightLeftToAndPattern : public OpRewritePattern<V_LSHLREV_B32> {
  using OpRewritePattern<V_LSHLREV_B32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_LSHLREV_B32 lshlOp,
                                PatternRewriter &rewriter) const override {
    // Check if src1 (value being shifted left) comes from a right shift
    auto lshrOp = lshlOp.getSrc1().getDefiningOp<V_LSHRREV_B32>();
    if (!lshrOp)
      return failure();

    // Both shift amounts must be the same constant
    auto lshlAmt = getConstantValue(lshlOp.getSrc0());
    auto lshrAmt = getConstantValue(lshrOp.getSrc0());
    if (!lshlAmt || !lshrAmt || *lshlAmt != *lshrAmt)
      return failure();

    // Shift amount must be valid
    if (*lshlAmt <= 0 || *lshlAmt >= 32)
      return failure();

    // Compute the AND mask: ~((1 << N) - 1) = all bits set except low N
    int64_t mask = ~((1LL << *lshlAmt) - 1) & 0xFFFFFFFF;

    auto loc = lshlOp.getLoc();
    auto resultType = lshlOp.getResult().getType();

    // Create the mask constant
    auto maskImm = rewriter.getType<ImmType>(mask);
    auto maskConst = ConstantOp::create(rewriter, loc, maskImm, mask);

    // Create: result = x & mask
    // NOTE: constant must be src0 (first operand) for VOP2 encoding.
    // src1 must be a VGPR on AMDGCN.
    auto andOp = V_AND_B32::create(rewriter, loc, resultType, maskConst,
                                   lshrOp.getSrc1());

    rewriter.replaceOp(lshlOp, andOp.getResult());
    return success();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: a + (0 - b) -> a - b
//
// The affine handler produces negate-then-add sequences:
//   %neg = v_sub_u32 0, %val
//   %result = v_add_u32 %other, %neg
// This simplifies to:
//   %result = v_sub_u32 %other, %val
// Saves 1 VALU instruction per occurrence.
//===----------------------------------------------------------------------===//

struct AddNegToSubPattern : public OpRewritePattern<V_ADD_U32> {
  using OpRewritePattern<V_ADD_U32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_ADD_U32 addOp,
                                PatternRewriter &rewriter) const override {
    auto checkOperand = [&](Value negated, Value other) -> LogicalResult {
      auto subOp = negated.getDefiningOp<V_SUB_U32>();
      if (!subOp)
        return failure();

      // Check that src0 is constant 0 (i.e., this is 0 - x = -x)
      auto zeroVal = getConstantValue(subOp.getSrc0());
      if (!zeroVal || *zeroVal != 0)
        return failure();

      // No single-use constraint: if the negation has multiple uses that are
      // all add(y, neg) patterns, the greedy rewriter will fire this pattern
      // for each use. Once all uses are rewritten, the negation becomes dead
      // and is cleaned up by DCE or the next CSE pass.

      auto loc = addOp.getLoc();
      auto resultType = addOp.getResult().getType();

      // Replace a + (0 - b) with a - b
      auto newSub =
          V_SUB_U32::create(rewriter, loc, resultType, other, subOp.getSrc1());

      rewriter.replaceOp(addOp, newSub.getResult());
      return success();
    };

    // Try both operand orderings
    if (succeeded(checkOperand(addOp.getSrc0(), addOp.getSrc1())))
      return success();
    if (succeeded(checkOperand(addOp.getSrc1(), addOp.getSrc0())))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: MFMA with zeroed accumulator -> MFMA with inline 0
//
// When the accumulator input to an MFMA is a zero constant (via v_mov_b32),
// replace it with an inline 0 constant directly. This:
// 1. Eliminates the zeroing mov instruction
// 2. Uses hardware's native inline constant support
//===----------------------------------------------------------------------===//

struct MFMAZeroAccumPattern : public RewritePattern {
  MFMAZeroAccumPattern(MLIRContext *ctx)
      : RewritePattern(MatchAnyOpTypeTag{}, /*benefit=*/1, ctx) {}

  LogicalResult matchAndRewrite(Operation *op,
                                PatternRewriter &rewriter) const override {
    // Check if this is an MFMA operation (has TiedOperandIndex attribute)
    // MFMA ops have 3 operands: a, b, acc
    if (op->getNumOperands() != 3 || op->getNumResults() != 1)
      return failure();

    // Check for MFMA ops by name pattern
    StringRef opName = op->getName().getStringRef();
    if (!opName.contains("mfma"))
      return failure();

    // Get the accumulator operand (operand index 2)
    Value accum = op->getOperand(2);

    // Check if accumulator comes from a v_mov_b32 of zero
    auto movOp = accum.getDefiningOp<V_MOV_B32>();
    if (!movOp)
      return failure();

    // Don't optimize away accumulator initializations marked with no_cse
    // These are loop-carried accumulators that need to exist as registers
    if (movOp->hasAttr("no_cse"))
      return failure();

    // Check if the mov source is a zero constant
    auto constOp = movOp.getSrc().getDefiningOp<ConstantOp>();
    if (!constOp || constOp.getValue() != 0)
      return failure();

    // Check that the mov result is only used by this MFMA
    if (!accum.hasOneUse())
      return failure();

    // Replace the accumulator with an inline zero constant
    auto loc = op->getLoc();
    auto immType = rewriter.getType<ImmType>(0);
    auto zeroConst = ConstantOp::create(rewriter, loc, immType, 0);

    // Update the MFMA's accumulator operand
    op->setOperand(2, zeroConst);

    return success();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Fold v_mov_b32 of constant into v_add_u32
//
// Transforms:
//   %tmp = v_mov_b32 %const  (where %const is a ConstantOp)
//   %dst = v_add_u32 %tmp, %x
// Into:
//   %dst = v_add_u32 %const, %x
//
// On GFX9, v_add_u32 (VOP2) supports a 32-bit literal constant as src0.
// This eliminates the v_mov_b32 instruction and frees its VGPR output,
// reducing both instruction count and register pressure.
//===----------------------------------------------------------------------===//

struct FoldMovConstIntoAddPattern : public OpRewritePattern<V_ADD_U32> {
  using OpRewritePattern<V_ADD_U32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_ADD_U32 addOp,
                                PatternRewriter &rewriter) const override {
    // Check if either operand is a v_mov_b32 of a constant
    auto tryFold = [&](Value movVal, Value other) -> LogicalResult {
      auto movOp = movVal.getDefiningOp<V_MOV_B32>();
      if (!movOp)
        return failure();

      // The mov source must be a ConstantOp (immediate)
      auto constOp = movOp.getSrc().getDefiningOp<ConstantOp>();
      if (!constOp)
        return failure();

      // Only fold if the mov result is used only by this add.
      // If other users exist, the mov is still needed.
      if (!movOp.getResult().hasOneUse())
        return failure();

      // Replace: v_add_u32(v_mov_b32(CONST), X) -> v_add_u32(CONST, X)
      auto loc = addOp.getLoc();
      auto resultType = addOp.getResult().getType();
      auto newAdd = V_ADD_U32::create(rewriter, loc, resultType,
                                      movOp.getSrc(), other);
      rewriter.replaceOp(addOp, newAdd.getResult());
      return success();
    };

    // Try both operand orderings (V_ADD_U32 is commutative)
    if (succeeded(tryFold(addOp.getSrc0(), addOp.getSrc1())))
      return success();
    if (succeeded(tryFold(addOp.getSrc1(), addOp.getSrc0())))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Fold v_mov_b32 of constant into v_sub_u32
//
// Transforms:
//   %tmp = v_mov_b32 %const
//   %dst = v_sub_u32 %x, %tmp     (x - const)
// Into:
//   %dst = v_sub_u32 %x, %const
//===----------------------------------------------------------------------===//

struct FoldMovConstIntoSubPattern : public OpRewritePattern<V_SUB_U32> {
  using OpRewritePattern<V_SUB_U32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_SUB_U32 subOp,
                                PatternRewriter &rewriter) const override {
    // Check each operand for a v_mov_b32 of a constant
    auto tryFoldSrc0 = [&]() -> LogicalResult {
      auto movOp = subOp.getSrc0().getDefiningOp<V_MOV_B32>();
      if (!movOp || !movOp.getSrc().getDefiningOp<ConstantOp>())
        return failure();
      if (!movOp.getResult().hasOneUse())
        return failure();
      auto loc = subOp.getLoc();
      auto resultType = subOp.getResult().getType();
      auto newSub = V_SUB_U32::create(rewriter, loc, resultType,
                                      movOp.getSrc(), subOp.getSrc1());
      rewriter.replaceOp(subOp, newSub.getResult());
      return success();
    };

    auto tryFoldSrc1 = [&]() -> LogicalResult {
      auto movOp = subOp.getSrc1().getDefiningOp<V_MOV_B32>();
      if (!movOp || !movOp.getSrc().getDefiningOp<ConstantOp>())
        return failure();
      if (!movOp.getResult().hasOneUse())
        return failure();
      auto loc = subOp.getLoc();
      auto resultType = subOp.getResult().getType();
      auto newSub = V_SUB_U32::create(rewriter, loc, resultType,
                                      subOp.getSrc0(), movOp.getSrc());
      rewriter.replaceOp(subOp, newSub.getResult());
      return success();
    };

    if (succeeded(tryFoldSrc0()))
      return success();
    if (succeeded(tryFoldSrc1()))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Fold v_mov_b32 of constant into v_lshlrev_b32
//
// Transforms:
//   %tmp = v_mov_b32 %const
//   %dst = v_lshlrev_b32 %tmp, %x    (x << const)
// Into:
//   %dst = v_lshlrev_b32 %const, %x
//===----------------------------------------------------------------------===//

struct FoldMovConstIntoLshlPattern : public OpRewritePattern<V_LSHLREV_B32> {
  using OpRewritePattern<V_LSHLREV_B32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_LSHLREV_B32 lshlOp,
                                PatternRewriter &rewriter) const override {
    // src0 is the shift amount - check if it's a v_mov_b32 of a constant
    auto movOp = lshlOp.getSrc0().getDefiningOp<V_MOV_B32>();
    if (!movOp)
      return failure();
    auto constOp = movOp.getSrc().getDefiningOp<ConstantOp>();
    if (!constOp)
      return failure();
    if (!movOp.getResult().hasOneUse())
      return failure();

    auto loc = lshlOp.getLoc();
    auto resultType = lshlOp.getResult().getType();
    auto newLshl = V_LSHLREV_B32::create(rewriter, loc, resultType,
                                          movOp.getSrc(), lshlOp.getSrc1());
    rewriter.replaceOp(lshlOp, newLshl.getResult());
    return success();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Fold v_mov_b32 of constant into v_lshrrev_b32
//===----------------------------------------------------------------------===//

struct FoldMovConstIntoLshrPattern : public OpRewritePattern<V_LSHRREV_B32> {
  using OpRewritePattern<V_LSHRREV_B32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_LSHRREV_B32 lshrOp,
                                PatternRewriter &rewriter) const override {
    auto movOp = lshrOp.getSrc0().getDefiningOp<V_MOV_B32>();
    if (!movOp)
      return failure();
    auto constOp = movOp.getSrc().getDefiningOp<ConstantOp>();
    if (!constOp)
      return failure();
    if (!movOp.getResult().hasOneUse())
      return failure();

    auto loc = lshrOp.getLoc();
    auto resultType = lshrOp.getResult().getType();
    auto newLshr = V_LSHRREV_B32::create(rewriter, loc, resultType,
                                          movOp.getSrc(), lshrOp.getSrc1());
    rewriter.replaceOp(lshrOp, newLshr.getResult());
    return success();
  }
};

//===----------------------------------------------------------------------===//
// Pattern: Fold v_mov_b32 of constant into v_and_b32
//===----------------------------------------------------------------------===//

struct FoldMovConstIntoAndPattern : public OpRewritePattern<V_AND_B32> {
  using OpRewritePattern<V_AND_B32>::OpRewritePattern;

  LogicalResult matchAndRewrite(V_AND_B32 andOp,
                                PatternRewriter &rewriter) const override {
    auto tryFold = [&](Value movVal, Value other) -> LogicalResult {
      auto movOp = movVal.getDefiningOp<V_MOV_B32>();
      if (!movOp)
        return failure();
      auto constOp = movOp.getSrc().getDefiningOp<ConstantOp>();
      if (!constOp)
        return failure();
      if (!movOp.getResult().hasOneUse())
        return failure();

      auto loc = andOp.getLoc();
      auto resultType = andOp.getResult().getType();
      auto newAnd = V_AND_B32::create(rewriter, loc, resultType,
                                      movOp.getSrc(), other);
      rewriter.replaceOp(andOp, newAnd.getResult());
      return success();
    };

    if (succeeded(tryFold(andOp.getSrc0(), andOp.getSrc1())))
      return success();
    if (succeeded(tryFold(andOp.getSrc1(), andOp.getSrc0())))
      return success();

    return failure();
  }
};

//===----------------------------------------------------------------------===//
// Peephole Optimization Pass
//===----------------------------------------------------------------------===//

struct PeepholePass
    : public PassWrapper<PeepholePass, OperationPass<ModuleOp>> {
  MLIR_DEFINE_EXPLICIT_INTERNAL_INLINE_TYPE_ID(PeepholePass)

  PeepholePass() = default;

  StringRef getArgument() const override { return "waveasm-peephole"; }

  StringRef getDescription() const override {
    return "Peephole optimizations for WAVEASM IR";
  }

  void runOnOperation() override {
    ModuleOp module = getOperation();
    MLIRContext *ctx = module.getContext();

    RewritePatternSet patterns(ctx);

    // Add all peephole patterns
    patterns.add<
        // clang-format off
        AddNegToSubPattern,
        AddZeroPattern,
        FoldMovConstIntoAddPattern,
        FoldMovConstIntoAndPattern,
        FoldMovConstIntoLshlPattern,
        FoldMovConstIntoLshrPattern,
        FoldMovConstIntoSubPattern,
        LshlAddPattern,
        LshlOrFusePattern,
        MFMAZeroAccumPattern,
        MulNegPow2ToShiftNegPattern,
        MulOnePattern,
        MulPow2ToShiftPattern,
        MulZeroPattern,
        RedundantMovePattern,
        ShiftRightLeftToAndPattern
        // clang-format on
        >(ctx);

    // Apply patterns greedily
    if (failed(applyPatternsGreedily(module, std::move(patterns)))) {
      signalPassFailure();
      return;
    }

    // Loop-invariant code motion (LICM): hoist VALU instructions whose
    // operands all dominate the loop out of LoopOp bodies.  This matches
    // the aiter pattern where all LDS address computation is in the
    // prologue and zero VALU appears in the hot loop.
    module.walk([](LoopOp loopOp) {
      Block &body = loopOp.getBodyBlock();
      Region *loopRegion = &loopOp.getBodyRegion();

      // Check if a value dominates the loop (defined outside the region)
      auto dominatesLoop = [&](Value val) -> bool {
        if (auto *defOp = val.getDefiningOp()) {
          return defOp->getParentRegion() != loopRegion;
        }
        // Block arguments of the loop body do NOT dominate (they're
        // loop-carried values that change each iteration)
        if (auto blockArg = dyn_cast<BlockArgument>(val)) {
          return blockArg.getOwner()->getParentOp() != loopOp.getOperation();
        }
        return false;
      };

      // Check if an op is a VALU (address computation) that's safe to hoist
      auto isHoistableVALU = [](Operation *op) -> bool {
        return isa<V_LSHLREV_B32, V_ADD_U32, V_MUL_LO_U32, V_AND_B32,
                   V_OR_B32, V_LSHL_ADD_U32, V_LSHRREV_B32>(op);
      };

      // Iteratively collect and hoist: after hoisting a batch, new ops
      // may become hoistable (their producer was just hoisted).
      bool changed = true;
      while (changed) {
        changed = false;
        SmallVector<Operation *> toHoist;
        for (Operation &op : body) {
          if (!isHoistableVALU(&op))
            continue;
          bool allDominate = true;
          for (Value operand : op.getOperands()) {
            if (!dominatesLoop(operand)) {
              allDominate = false;
              break;
            }
          }
          if (allDominate)
            toHoist.push_back(&op);
        }
        for (Operation *op : toHoist) {
          op->moveBefore(loopOp);
          changed = true;
        }
      }
    });

    // After LICM and pattern-based optimizations, eliminate redundant M0
    // writes. Track the last M0 source value and skip writes that set M0
    // to the same value. This requires sequential analysis, not a
    // rewrite pattern.
    module.walk([](ProgramOp program) {
      program.walk([](Block *block) {
        Value lastM0Source;
        SmallVector<Operation *, 8> toErase;

        for (Operation &op : *block) {
          auto m0Op = dyn_cast<S_MOV_B32_M0>(&op);
          if (!m0Op)
            continue;

          if (m0Op->getNumOperands() < 1)
            continue;

          Value src = m0Op->getOperand(0);

          // If M0 already holds this value, the write is redundant
          if (src == lastM0Source) {
            toErase.push_back(m0Op);
          } else {
            lastM0Source = src;
          }
        }

        for (auto *op : toErase)
          op->erase();
      });
    });
  }
};

} // namespace

namespace waveasm {

std::unique_ptr<mlir::Pass> createWAVEASMPeepholePass() {
  return std::make_unique<PeepholePass>();
}

} // namespace waveasm
