# Copyright 2025 The IREE Authors
#
# Licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

import functools
from typing import Any, Optional

import math
import sympy
import torch.fx as fx

from wave_lang.kernel.wave.utils.graph_utils import propagate_loop_carried_vars
from wave_lang.support.ir_imports import (
    Attribute,
    DenseElementsAttr,
    IndexType,
    InsertionPoint,
    IntegerAttr,
    IntegerType,
    IrType,
    MemRefType,
    OpResult,
    ShapedType,
    Value,
    VectorType,
    amdgpu_d,
    arith_d,
    func_d,
    gpu_d,
    llvm_d,
    memref_d,
    vector_d,
)
from .ir_utils import (
    is_float_type,
)

from ..._support.indexing import (
    IndexExpr,
    IndexingContext,
    IndexSequence,
    IndexSymbol,
    subs_idxc,
)
from ..base import ValidationError
from ..builder import IRProxyValue
from ..utils import strides_from_symbolic_shape
from ...lang.global_symbols import *
from ...lang.wave_types import IndexMapping
from ...ops.wave_ops import (
    CustomOp,
    gather_to_lds,
    tensor_load_to_lds,
    get_custom,
    read,
    write,
    scatter_add,
    read_meets_hw_transpose_requirements,
    MemoryAccessFlags,
)
from ...wave.utils.general_utils import get_fastest_index, infer_dim, linearize_index
from ...wave.utils.mapping_utils import transform_index_on_mapping
from ...wave.utils.symbol_utils import safe_subs
from .emitter import (
    WaveEmitter,
    add_emitter_subs,
    cast_kernel_buffer,
    cast_py_literal,
    cast_py_value,
    cast_vector,
    gen_sympy_index,
    get_constant_attr,
    get_type_or_element_type,
    handle_op,
)


def _get_start_index(i: IndexSequence | IndexExpr) -> IndexExpr:
    if isinstance(i, IndexSequence):
        i = i.start

    return i


def _get_start_indices(
    src_indices: dict[IndexExpr, IndexSequence | IndexExpr],
) -> list[IndexExpr]:
    start_indices = []
    for dim_indexing in src_indices:
        i = _get_start_index(src_indices[dim_indexing])
        start_indices.append(i)

    return start_indices


@functools.lru_cache
def _simplify(expr):
    """
    Simple wrapper around simplify in order to utilize LRU Cache.
    This is important to minimize compile time caused by re-simplifying
    expressions.
    """
    return sympy.simplify(expr)


def _split_index(src: IndexExpr | int) -> tuple[IndexExpr, IndexExpr]:
    """
    Split index expr into thread-dependent and thread-independent parts
    """
    subs_wg = {
        WORKGROUP_0: 0,
        WORKGROUP_1: 0,
        WORKGROUP_2: 0,
        WAVE_ID_0: 0,
        WAVE_ID_1: 0,
        WAVE_ID_2: 0,
    }
    # Replace all wg symbols with 0s to get thread-dependent index.
    # All dynamic values will also be part of thread-index.
    thread_dependent_index = safe_subs(src, subs_wg)

    # Compute thread-independent index as `orig_index - thread_dependent_index`
    # All thread symbols and dynamic should cancel-out in the result.
    thread_independent_index = _simplify(src - thread_dependent_index)
    if thread_independent_index.free_symbols - set(subs_wg.keys()):
        # If we have any symbols besides wg symbols, means some thread or
        # dynamic symbols were not canceled out, use the entire index as
        # thread dependent index.
        thread_independent_index = sympy.sympify(0)
        thread_dependent_index = src

    return thread_independent_index, thread_dependent_index


def _extract0(src):
    static_pos = [0] * src.type.rank
    return vector_d.extract(src, static_position=static_pos, dynamic_position=[])


def _build_dyn_vals_map(
    mapping: Optional[IndexMapping], dynamic_vals: tuple[Value, ...]
) -> dict[IndexExpr, Value]:
    if mapping is None:
        return {}

    assert len(mapping.dynamic_val_indices) == len(
        dynamic_vals
    ), f"Expected {len(mapping.dynamic_val_indices)} dynamic values but got {len(dynamic_vals)}"
    return {
        sym: _extract0(val)
        for sym, val in zip(mapping.dynamic_val_indices.keys(), dynamic_vals)
    }


def _build_start_indices(
    emitter: WaveEmitter,
    src_indices: dict[IndexExpr, IndexSequence | IndexExpr],
    dynamic_values: dict[IndexExpr, Any] = {},
) -> tuple[list[OpResult], list[OpResult], list[OpResult]]:
    start_indices = _get_start_indices(src_indices)
    split_indices = [_split_index(i) for i in start_indices]
    subs = add_emitter_subs(emitter, dynamic_values)
    indices = [gen_sympy_index(subs, i) for i in start_indices]
    indices_wg = [gen_sympy_index(subs, i[0]) for i in split_indices]
    indices_th = [gen_sympy_index(subs, i[1]) for i in split_indices]

    return indices, indices_wg, indices_th


def _get_symbolic_shape(node: fx.Node) -> tuple[IndexExpr]:
    return get_custom(node).type.symbolic_shape


def _build_mask(
    emitter: WaveEmitter,
    index: dict[IndexExpr, IndexExpr],
    elements_per_thread: int,
    bounds: Optional[dict[IndexSymbol, IndexExpr]],
    dynamic_values: dict[IndexExpr, Any] = {},
) -> Optional[OpResult]:
    if not bounds:
        return None

    idxc = IndexingContext.current()
    fastest_dim = get_fastest_index(index)
    last_dim = list(index)[fastest_dim]
    new_index = {k: _get_start_index(v) for k, v in index.items()}

    new_index[last_dim] = new_index[last_dim] + idxc.iota(elements_per_thread)

    mask_expr = functools.reduce(
        lambda a, b: sympy.And(a, b),
        (new_index[dim] < bound for dim, bound in bounds.items()),
    )
    mask = gen_sympy_index(add_emitter_subs(emitter, dynamic_values), mask_expr)

    mask_vec_type = VectorType.get([elements_per_thread], IntegerType.get_signless(1))
    if mask.type != mask_vec_type:
        mask = vector_d.broadcast(mask_vec_type, mask)

    return mask


def _get_splat_const(vec_type: IrType, value: Any) -> Value:
    splat = DenseElementsAttr.get_splat(
        vec_type, get_constant_attr(value, vec_type.element_type)
    )
    return arith_d.constant(vec_type, splat)


def _constant_mask(vec_type: IrType) -> Value:
    return _get_splat_const(vec_type, 1)


def _get_max_buffer_size(elem_type: IrType) -> int:
    """
    Return max memref size suitable for buffer ops.

    Buffer ops offsets are i32, return maximum memref size in elements.
    """
    return ((1 << 31) - 1) // (elem_type.width // 8)


def _linearize_memref(
    mem: Value,
    offsets_wg: tuple[Value | int],
    offsets_th: tuple[Value | int],
    strides: tuple[Value],
) -> tuple[Value, Value]:
    """
    Convert n-D memref into 1-D memref, suitable for buffer ops.

    Apply offsets to the memref and convert result to 1-D. Resulting memref size
    is set to `max_buffer_size - 1` so buffer access to the last element will be
    no-op.
    """
    memref_type = mem.type
    offset = None
    offset_th = None
    overflow_flags = arith_d.IntegerOverflowFlags.nsw
    for ind_wg, ind_th, stride in zip(offsets_wg, offsets_th, strides):
        if isinstance(ind_wg, int):
            ind_wg = arith_d.constant(IndexType.get(), ind_wg)

        if isinstance(ind_th, int):
            ind_th = arith_d.constant(IndexType.get(), ind_th)

        off_wg = arith_d.muli(ind_wg, stride, overflow_flags=overflow_flags)
        if offset is None:
            offset = off_wg
        else:
            offset = arith_d.addi(offset, off_wg, overflow_flags=overflow_flags)

        off_th = arith_d.muli(ind_th, stride, overflow_flags=overflow_flags)
        if offset_th is None:
            offset_th = off_th
        else:
            offset_th = arith_d.addi(offset_th, off_th, overflow_flags=overflow_flags)

    size_full = arith_d.constant(
        IndexType.get(), _get_max_buffer_size(memref_type.element_type) - 1
    )

    dyn_val = ShapedType.get_dynamic_size()
    res_shape = [dyn_val]
    element_type = memref_type.element_type
    memory_space = memref_type.memory_space
    resut_type = MemRefType.get(
        res_shape,
        element_type,
        layout=Attribute.parse("strided<[1], offset: ?>"),
        memory_space=memory_space,
    )
    memref_metadata = memref_d.extract_strided_metadata(mem)
    memref_base_offset = memref_metadata[1]
    offset = arith_d.addi(offset, memref_base_offset, overflow_flags=overflow_flags)
    return (
        memref_d.reinterpret_cast(
            resut_type,
            mem,
            offsets=[offset],
            sizes=[size_full],
            strides=[],
            static_offsets=[dyn_val],
            static_sizes=[dyn_val],
            static_strides=[1],
        ),
        offset_th,
    )


def _linearize_shared_mem(memory: CustomOp) -> Value:
    """
    Convert shared memory with statically shaped N-d memref into 1-D memref.
    """
    flat_numel = math.prod(memory.type.shape)
    assert (
        memory.type.has_static_shape
    ), "Expecting static shape to linearize for shared memory."
    memory_space = memory.type.memory_space
    flat_memref_type = MemRefType.get(
        [flat_numel], memory.type.element_type, memory_space=memory_space
    )
    flattened_mem = memref_d.reinterpret_cast(
        flat_memref_type,
        memory,
        offsets=[],
        sizes=[],
        strides=[],
        static_offsets=[0],
        static_sizes=[flat_numel],
        static_strides=[1],
    )
    return flattened_mem


def _get_splat_input(src: Optional[Value]) -> Optional[Value]:
    """
    If `src` is vector.splat result, return splat input, otherwise return None.
    """
    if src is None:
        return None

    owner = getattr(src, "owner", None)
    if owner is None:
        return None

    op = src.owner.opview
    if isinstance(op, vector_d.BroadcastOp) and not isinstance(
        op.source.type, VectorType
    ):
        return op.source

    return None


def _valid_bytes_buffer(elem_type: IrType) -> int:
    """
    Make valid bytes to be the address of the last byte of the second to last element that can fit in a 32 bit offset to memory address
    """
    ans = (1 << 31) - 1 - (elem_type.width // 8)

    assert isinstance(ans, int)
    return ans


def _get_out_of_bounds_index(element_type: IrType) -> int:
    """
    returns the first index that's out of bounds of a buffer based on the element type and maximum bytes
    """
    element_width_in_bytes = element_type.width // 8
    oob_index_value = (
        _valid_bytes_buffer(element_type) + element_width_in_bytes
    ) // element_width_in_bytes
    assert (oob_index_value * element_width_in_bytes) > _valid_bytes_buffer(
        element_type
    )
    assert (oob_index_value * element_width_in_bytes) < (1 << 31)
    return oob_index_value


def _get_constant_value(candidate: Value):
    """
    returns constantOp's value if candidate is arith.constantOp. Else, returns None.
    """
    if not hasattr(candidate.owner, "opview") or not isinstance(
        candidate.owner.opview, arith_d.ConstantOp
    ):
        return None
    return candidate.owner.opview.value.value


def _cast_buffer_and_encode_stride(
    ptr: Value, strides: tuple[Value], elem_type: IrType, emitter: WaveEmitter
) -> Value:
    uint64 = IntegerType.get_signless(64)
    uint14 = IntegerType.get_signless(14)

    valid_bytes = _valid_bytes_buffer(
        elem_type
    )  # max bytes that are in range to be addressed from a buffer
    valid_bytes_constant = get_constant_attr(valid_bytes, uint64)
    valid_bytes_constant = arith_d.constant(uint64, valid_bytes_constant)
    stride_rank = len(strides)
    swizzle_stride = None

    if stride_rank >= 2:
        # fastest_dim_bound == second to last stride.
        stride_candidate = strides[-2]
        stride_int = _get_constant_value(stride_candidate)
        # Only swizzle if stride is static and <= 8192(the useful case).
        if stride_int and stride_int <= 8192:
            swizzle_stride = arith_d.index_cast(uint14, stride_candidate)

    if swizzle_stride:
        ptr = amdgpu_d.fat_raw_buffer_cast(
            ptr,
            cache_swizzle_stride=swizzle_stride,
            bounds_check=True,
            reset_offset=True,
            valid_bytes=valid_bytes_constant,
        )
    else:
        ptr = amdgpu_d.fat_raw_buffer_cast(
            ptr,
            bounds_check=True,
            reset_offset=True,
            valid_bytes=valid_bytes_constant,
        )

    return ptr


def _create_llvm_read_write(
    kb_mem: Value,
    kb_ir_type: MemRefType,
    start_indices: tuple[Value],
    vector_type: VectorType,
    flags: MemoryAccessFlags,
    value: Optional[Value] = None,
) -> Optional[Value]:
    is_read = value is None
    element_type = vector_type.element_type

    ptr = memref_d.extract_aligned_pointer_as_index(kb_mem)
    strides, _ = kb_ir_type.get_strides_and_offset()
    offset = arith_d.constant(IndexType.get(), 0)
    elem_size_bytes = element_type.width // 8

    for idx, stride in zip(start_indices, strides):
        if not isinstance(idx.type, IndexType):
            idx = arith_d.index_cast(IndexType.get(), idx)
        stride_val = arith_d.constant(IndexType.get(), stride * elem_size_bytes)
        stride_offset = arith_d.muli(idx, stride_val)
        offset = arith_d.addi(offset, stride_offset)

    final_ptr_index = arith_d.addi(ptr, offset)
    i64 = IntegerType.get_signless(64)
    final_ptr_i64 = arith_d.index_cast(i64, final_ptr_index)

    llvm_ptr_type = llvm_d.PointerType.get()
    llvm_ptr = llvm_d.IntToPtrOp(llvm_ptr_type, final_ptr_i64).result

    volatile_ = bool(flags & MemoryAccessFlags.VOLATILE)
    nontemporal = bool(flags & MemoryAccessFlags.NONTEMPORAL)

    if is_read:
        return llvm_d.LoadOp(
            vector_type,
            llvm_ptr,
            volatile_=volatile_,
            nontemporal=nontemporal,
        ).result
    else:
        llvm_d.StoreOp(
            value,
            llvm_ptr,
            volatile_=volatile_,
            nontemporal=nontemporal,
        )
        return None


def _create_vec_read_write(
    emitter: WaveEmitter,
    symbolic_shape: tuple[IndexExpr, ...],
    mem: Value,
    value: Optional[Value],
    vector_type: Optional[IrType],
    start_indices: tuple[Value],
    start_indices_wg: tuple[Value],
    start_indices_th: tuple[Value],
    elements_per_thread: int,
    memory: CustomOp,
    mask: Optional[Value],
    node_index: Optional[IndexSequence] = None,
) -> Optional[Value]:
    is_read = value is None
    uint32 = IntegerType.get_signless(32)

    def extract(vec, ind):
        return vector_d.extract(vec, static_position=[ind], dynamic_position=[])

    if memory.type.address_space == SHARED_ADDRESS_SPACE and hasattr(
        memory, "distributed_shape"
    ):
        symbolic_shape = memory.distributed_shape

    # only use buffer ops on global memory
    is_global_mem = mem.type.memory_space is None
    buffer_ops_enabled = emitter.options.use_buffer_ops and is_global_mem
    is_shared_mem = memory.type.address_space == SHARED_ADDRESS_SPACE and node_index
    linearize_shared_mem = is_shared_mem and emitter.options.linearize_shared_access

    stride_values = strides_from_symbolic_shape(
        IndexingContext.current(), symbolic_shape, allow_mixed_shapes=True
    )
    has_int_strides = all(isinstance(s, int) for s in stride_values)
    strides = [gen_sympy_index(add_emitter_subs(emitter), s) for s in stride_values]

    no_masked_load_store_ops = buffer_ops_enabled

    mask_splat = _get_splat_input(mask)
    splatted_mask = mask_splat is not None

    if vector_type is None:
        vector_type = value.type

    element_type = vector_type.element_type
    # Case 1: Generate load/stores with no mask and no offset
    if mask is None:
        offset_th = None
        if buffer_ops_enabled:
            # TODO: If strides cannot be converted into integers, means they are dynamic
            # and linearize breaks, need to investigate later.
            mem, offset_th = _linearize_memref(
                mem, start_indices_wg, start_indices_th, strides
            )
            mem = _cast_buffer_and_encode_stride(mem, strides, element_type, emitter)
        elif is_global_mem and not is_read:
            mem, offset_th = _linearize_memref(
                mem, start_indices_wg, start_indices_th, strides
            )
        if linearize_shared_mem:
            mem = _linearize_shared_mem(mem)
            linearized_index = {
                "linearized_idx": linearize_index(node_index, stride_values)
            }
            start_indices, _, _ = _build_start_indices(emitter, linearized_index)

        indices = (
            [offset_th]
            if (buffer_ops_enabled or offset_th is not None)
            else start_indices
        )
        if is_read:
            return vector_d.load(vector_type, mem, indices)
        else:
            vector_d.store(value, mem, indices)
            return

    zero = get_constant_attr(0, element_type)
    zero = arith_d.constant(element_type, zero)

    if mask is None:
        mask_vec_type = VectorType.get(
            [elements_per_thread], IntegerType.get_signless(1)
        )
        mask = _constant_mask(mask_vec_type)

    # make offsets 0, 1, 2 ...
    offsets_vec_type = VectorType.get(vector_type.shape, IndexType.get())
    vals = [IntegerAttr.get(IndexType.get(), v) for v in range(elements_per_thread)]

    offsets_vec = arith_d.constant(
        offsets_vec_type, DenseElementsAttr.get(vals, offsets_vec_type)
    )

    offset_th = None
    if buffer_ops_enabled:
        mem, offset_th = _linearize_memref(
            mem, start_indices_wg, start_indices_th, strides
        )
        mem = _cast_buffer_and_encode_stride(mem, strides, element_type, emitter)

    indices = [offset_th] if buffer_ops_enabled else start_indices

    if no_masked_load_store_ops:
        # find the index at which memory out of bounds of buffer
        oob_index_value = _get_out_of_bounds_index(element_type)
        oob_index = arith_d.constant(IndexType.get(), oob_index_value)

        oob_index = vector_d.broadcast(
            VectorType.get(vector_type.shape, IndexType.get()), oob_index
        )

        offset_th = vector_d.broadcast(
            VectorType.get(vector_type.shape, IndexType.get()), offset_th
        )

        uint32_vec_type = VectorType.get([elements_per_thread], uint32)
        indexvec_type = VectorType.get([elements_per_thread], IndexType.get())

        offsets_vec = arith_d.index_cast(uint32_vec_type, offsets_vec)
        offset_th = arith_d.index_cast(uint32_vec_type, offset_th)

        # add the thread offset and the vec offsets
        offsets_vec = arith_d.addi(offsets_vec, offset_th)
        offsets_vec = arith_d.index_cast(indexvec_type, offsets_vec)

        # based on mask, select between the offsets_vec and out of bounds. In this case all 3 operands can be vectors
        selected_index = arith_d.select(mask, offsets_vec, oob_index)
        elems = list()

        if splatted_mask:
            # mask is same for all of them, can just pick the first index
            selected_index = extract(selected_index, 0)

            if is_read:
                return vector_d.load(vector_type, mem, indices=[selected_index])

            else:
                vector_d.store(value, mem, indices=[selected_index])
                return

        for i in range(elements_per_thread):
            # mask is not same for all elements, need to unroll
            this_index = extract(selected_index, i)  # this element

            # Unmasked load, using selected_index
            singlenumvec_type = VectorType.get([1], vector_type.element_type)
            if is_read:
                elem = vector_d.load(singlenumvec_type, mem, indices=[this_index])
                elem = extract(elem, 0)
                elems.append(elem)
            else:
                elem = extract(value, i)
                single_num_vector = vector_d.broadcast(singlenumvec_type, elem)
                vector_d.store(single_num_vector, mem, indices=[this_index])

        if is_read:
            # now make a vector from all the elements loaded
            return vector_d.from_elements(vector_type, elems)

        else:  # it was a store, return
            return

    else:
        # normal masked load/store

        if is_read:
            passthru = vector_d.broadcast(vector_type, zero)
            return vector_d.maskedload(vector_type, mem, indices, mask, passthru)
        else:
            vector_d.maskedstore(mem, indices, mask, value)
            return


_WAVEASM_UNIFORM = {
    WORKGROUP_0: 0,
    WORKGROUP_1: 0,
    WORKGROUP_2: 0,
    THREAD_1: 0,
    THREAD_2: 0,
    WAVE_ID_0: 0,
    WAVE_ID_1: 0,
    WAVE_ID_2: 0,
}
_WAVEASM_UNIFORM_KEYS = set(_WAVEASM_UNIFORM.keys())


_linearize_cache: dict = {}


def _linearize_read_waveasm(
    emitter: WaveEmitter,
    mem: Value,
    node_index: Optional[dict],
    dynamic_values: dict[IndexExpr, Any],
    symbolic_shape: tuple[IndexExpr, ...],
) -> tuple[Value, Value]:
    """
    Linearize a global read for the WaveASM backend, treating THREAD_1/2
    as wave-uniform (SRD base) so the per-lane voffset depends only on
    THREAD_0.  Returns (linearized_mem, th_offset).
    """
    kb_type = MemRefType(mem.type)
    phys_strides, _ = kb_type.get_strides_and_offset()
    overflow_flags = arith_d.IntegerOverflowFlags.nsw
    start_exprs = _get_start_indices(node_index)
    subs = add_emitter_subs(emitter, dynamic_values)

    wg_offset = None
    th_offset = None
    for expr, ps in zip(start_exprs, phys_strides):
        th_expr = safe_subs(expr, _WAVEASM_UNIFORM)
        wg_expr = sympy.expand(expr - th_expr)
        if wg_expr.free_symbols - _WAVEASM_UNIFORM_KEYS:
            wg_expr = sympy.sympify(0)
            th_expr = expr

        wg_val = gen_sympy_index(subs, wg_expr)
        th_val = gen_sympy_index(subs, th_expr)
        stride_val = arith_d.constant(IndexType.get(), ps)

        wg_term = arith_d.muli(wg_val, stride_val, overflow_flags=overflow_flags)
        th_term = arith_d.muli(th_val, stride_val, overflow_flags=overflow_flags)
        wg_offset = (
            wg_term
            if wg_offset is None
            else arith_d.addi(wg_offset, wg_term, overflow_flags=overflow_flags)
        )
        th_offset = (
            th_term
            if th_offset is None
            else arith_d.addi(th_offset, th_term, overflow_flags=overflow_flags)
        )

    if not hasattr(emitter, "_linearize_cache"):
        emitter._linearize_cache = {}
    cache_key = mem
    if cache_key in emitter._linearize_cache:
        return emitter._linearize_cache[cache_key], th_offset

    max_buf = _get_max_buffer_size(kb_type.element_type) - 1
    dyn_val = ShapedType.get_dynamic_size()
    result_type = MemRefType.get(
        [max_buf],
        kb_type.element_type,
        layout=Attribute.parse("strided<[1], offset: ?>"),
        memory_space=kb_type.memory_space,
    )
    linearized_mem = memref_d.reinterpret_cast(
        result_type,
        mem,
        offsets=[wg_offset],
        sizes=[],
        strides=[],
        static_offsets=[dyn_val],
        static_sizes=[max_buf],
        static_strides=[1],
    )
    emitter._linearize_cache[cache_key] = linearized_mem
    return linearized_mem, th_offset


def _get_or_create_flat_memref(
    emitter: WaveEmitter,
    mem: Value,
) -> Value:
    """Return a rank-1 view of *mem* with offset 0 (pure shape change).

    All reads from the same source buffer share one reinterpret_cast,
    so the backend maps them all to a single SRD — no per-read SRD copies.
    """
    if not hasattr(emitter, "_flat_memref_cache"):
        emitter._flat_memref_cache = {}
    key = id(mem)
    if key in emitter._flat_memref_cache:
        return emitter._flat_memref_cache[key]

    kb_type = MemRefType(mem.type)
    max_buf = _get_max_buffer_size(kb_type.element_type) - 1
    result_type = MemRefType.get(
        [max_buf],
        kb_type.element_type,
        layout=Attribute.parse("strided<[1], offset: 0>"),
        memory_space=kb_type.memory_space,
    )
    flat = memref_d.reinterpret_cast(
        result_type,
        mem,
        offsets=[],
        sizes=[],
        strides=[],
        static_offsets=[0],
        static_sizes=[max_buf],
        static_strides=[1],
    )
    emitter._flat_memref_cache[key] = flat
    return flat


def _emit_iv_split_read(
    emitter: WaveEmitter,
    node: fx.Node,
    index: dict[IndexExpr, IndexSequence | IndexExpr],
    kb_src: Value,
    input_shape: tuple[IndexExpr, ...],
    vector_type: VectorType,
    dynamic_vals_map_start: dict[IndexExpr, Any],
) -> Optional[Value]:
    """
    Emit a VALU-free global read inside a tiled loop.

    Follows the AITER methodology:
      1. ONE shared rank-1 memref per source buffer (no per-read SRD copies).
      2. Full linearized offset at IV=0 → voffset, hoisted before the loop.
      3. IV * k_stride added inside loop → BufferLoadStrengthReduction
         promotes it to soffset, yielding zero in-loop VALU.

    Uses a 3-point linearity check on the post-mapping codegen-time indices
    so it works even when the pre-codegen pass couldn't tag the node.
    """
    iv_vals, iv_syms = emitter.get_induction_vars_and_syms()
    if not iv_syms:
        return None

    kb_type = MemRefType(kb_src.type)
    if kb_type.rank == 0:
        return None

    ip = InsertionPoint.current
    owner = ip.block.owner
    if isinstance(owner, func_d.FuncOp):
        return None

    # --- Determine k_stride_per_iv ---
    if getattr(node, "iv_linear", False):
        k_stride_per_iv = node.iv_k_stride
    else:
        phys_strides, _ = kb_type.get_strides_and_offset()
        dyn_sentinel = ShapedType.get_dynamic_stride_or_offset()
        if any(s == dyn_sentinel for s in phys_strides):
            return None

        step_int = _get_constant_value(owner.operands[2])
        if step_int is None or step_int <= 0:
            return None

        start_exprs = _get_start_indices(index)
        if len(start_exprs) != len(phys_strides):
            return None

        all_zero = {
            THREAD_0: 0,
            THREAD_1: 0,
            THREAD_2: 0,
            WORKGROUP_0: 0,
            WORKGROUP_1: 0,
            WORKGROUP_2: 0,
            WAVE_ID_0: 0,
            WAVE_ID_1: 0,
            WAVE_ID_2: 0,
        }
        iv_sym = iv_syms[0]
        try:
            d1 = d2 = 0
            for expr, ps in zip(start_exprs, phys_strides):
                v0 = int(safe_subs(expr, {**all_zero, iv_sym: 0}))
                v1 = int(safe_subs(expr, {**all_zero, iv_sym: step_int}))
                v2 = int(safe_subs(expr, {**all_zero, iv_sym: 2 * step_int}))
                d1 += (v1 - v0) * ps
                d2 += (v2 - v1) * ps
        except (TypeError, ValueError, sympy.SympifyError):
            return None

        if d1 != d2 or d1 == 0:
            return None
        k_stride_per_iv, rem = divmod(d1, step_int)
        if rem != 0:
            return None

    # --- Zero IV in index expressions ---
    iv_zero_subs = {sym: 0 for sym in iv_syms}
    index_no_iv = {}
    for dim, seq in index.items():
        start = _get_start_index(seq)
        new_start = safe_subs(start, iv_zero_subs)
        if isinstance(seq, IndexSequence):
            index_no_iv[dim] = IndexSequence(new_start, seq.size)
        else:
            index_no_iv[dim] = new_start

    # --- Hoist: compute full linearized voffset at IV=0, create shared flat memref ---
    kb_type = MemRefType(kb_src.type)
    phys_strides, _ = kb_type.get_strides_and_offset()
    hoist_ip = InsertionPoint(owner)
    subs_map = add_emitter_subs(emitter, dynamic_vals_map_start)
    overflow_flags = arith_d.IntegerOverflowFlags.nsw

    with hoist_ip:
        flat_mem = _get_or_create_flat_memref(emitter, kb_src)

        iv0_exprs = _get_start_indices(index_no_iv)
        lin_offset = None
        for expr, ps in zip(iv0_exprs, phys_strides):
            val = gen_sympy_index(subs_map, expr)
            stride_c = arith_d.constant(IndexType.get(), ps)
            term = arith_d.muli(val, stride_c, overflow_flags=overflow_flags)
            lin_offset = (
                term
                if lin_offset is None
                else arith_d.addi(lin_offset, term, overflow_flags=overflow_flags)
            )

    # --- In-loop: total = hoisted_voffset + IV * k_stride ---
    iv_sym = iv_syms[0]
    iv_mlir = subs_map.get(iv_sym)
    if iv_mlir is None:
        return None

    k_stride_val = gen_sympy_index(subs_map, sympy.sympify(k_stride_per_iv))
    iv_offset = arith_d.muli(iv_mlir, k_stride_val, overflow_flags=overflow_flags)
    total_offset = arith_d.addi(lin_offset, iv_offset, overflow_flags=overflow_flags)

    return vector_d.load(vector_type, flat_mem, [total_offset])


def _build_mask_with_mapping(
    emitter: WaveEmitter,
    mapping: IndexMapping,
    index: dict[IndexSymbol, IndexSequence],
    transformed_index: dict[IndexSymbol, IndexSequence],
    memory_shape: tuple[IndexSymbol, ...],
    elements_per_thread: int,
    bounds: Optional[tuple[IndexSymbol, ...]],
    dynamic_vals_map: dict[IndexExpr, Value],
) -> Optional[Value]:
    """
    Build a mask for read/write operations, when a mapping is used

    Either build the mask w/ the original index or transformed index
    We want to build the mask w/ the transformed index when
      - the transformed_index has the same dimensions in bounds for correct masking
      - no dynamic_val_indices are used in the mapping
      - memory dims are not dynamic values
    This matches the case when the original index can be transformed within the mapping itself i.e.

    tkw.IndexMapping(num_iterators=2, inputs={M: i + CTA_M_OFFSET, K: j}, outputs={M: i, K: j},)

    So the transformed index: "i + OFFSET" must be passed into the masking first
    else the original index is passed into the masking first if it is not changed within the mapping

    """
    static_memory_dims = not any(dim in emitter.dynamic_dims for dim in memory_shape)
    use_transformed_index = (
        bounds
        and all(dim in transformed_index for dim in bounds)
        and not mapping.dynamic_val_indices
        and static_memory_dims
    )
    if use_transformed_index:
        return _build_mask(
            emitter,
            transformed_index,
            elements_per_thread,
            bounds,
            dynamic_vals_map,
        )
    else:
        return _build_mask(emitter, index, elements_per_thread, bounds)


@handle_op(read)
def handle_read(emitter: WaveEmitter, node: fx.Node):
    # This is similar to tkl.store with fixed start indices for now.
    try:
        memory, elements_per_thread, mapping, dyn_vals, bounds, flags, *rest = node.args
    except ValueError as e:
        raise ValidationError("Malformed arguments") from e

    vector_shape = cast_py_literal(emitter, (elements_per_thread,))
    # memory has no IR node yet.
    kb_src, kb_ir_type, kb_py_type = cast_kernel_buffer(emitter, memory)

    if not hasattr(node, "index"):
        raise ValidationError("codegen expected read to have index attr.")

    index = node.index

    element_type = kb_ir_type.element_type
    vector_type = VectorType.get(vector_shape, element_type)
    input_shape = _get_symbolic_shape(memory)
    elements_per_thread = cast_py_literal(emitter, elements_per_thread)
    dyn_vals = tuple(
        cast_vector(emitter, reg, element_type=IndexType.get()) for reg in dyn_vals
    )
    dynamic_vals_map_start = _build_dyn_vals_map(mapping, dyn_vals)

    if mapping:
        transformed_index = transform_index_on_mapping(
            mapping, input_shape, index, is_read=True
        )
        mask = _build_mask_with_mapping(
            emitter,
            mapping,
            index,
            transformed_index,
            input_shape,
            elements_per_thread,
            bounds,
            dynamic_vals_map_start,
        )
        index = transformed_index
    else:
        mask = _build_mask(emitter, index, elements_per_thread, bounds)

    is_global = get_custom(memory).type.address_space != SHARED_ADDRESS_SPACE
    use_llvm_load = flags != MemoryAccessFlags.NONE

    if (
        is_global
        and mask is None
        and not use_llvm_load
        and emitter.options.use_wave_asm_backend
        and not read_meets_hw_transpose_requirements(
            get_custom(node), emitter.constraints, emitter.options.target
        )
    ):
        result = _emit_iv_split_read(
            emitter,
            node,
            index,
            kb_src,
            input_shape,
            vector_type,
            dynamic_vals_map_start,
        )
        if result is not None:
            emitter.bind_node_proxy(node, IRProxyValue(result))
            return

    start_indices, start_indices_wg, start_indices_th = _build_start_indices(
        emitter, index, dynamic_vals_map_start
    )

    if use_llvm_load:
        result = _create_llvm_read_write(
            kb_src, kb_ir_type, start_indices, vector_type, flags
        )
    elif read_meets_hw_transpose_requirements(
        get_custom(node), emitter.constraints, emitter.options.target
    ):
        result = amdgpu_d.transpose_load(vector_type, kb_src, start_indices)
    else:
        result = _create_vec_read_write(
            emitter,
            input_shape,
            kb_src,
            None,
            vector_type,
            start_indices,
            start_indices_wg,
            start_indices_th,
            elements_per_thread,
            get_custom(memory),
            mask,
            node_index=index,
        )

    emitter.bind_node_proxy(node, IRProxyValue(result))


@handle_op(write)
def handle_write(emitter: WaveEmitter, node: fx.Node):
    try:
        (
            register,
            memory,
            elements_per_thread,
            mapping,
            dyn_vals,
            bounds,
            flags,
            *rest,
        ) = node.args
    except ValueError as e:
        raise ValidationError("Malformed arguments") from e

    # memory has no IR node yet.
    kb_dest, kb_ir_type, kb_py_type = cast_kernel_buffer(emitter, memory)
    insert_vector = cast_vector(emitter, register, element_type=kb_ir_type.element_type)
    insert_type = VectorType(insert_vector.type)
    vector_shape = cast_py_literal(emitter, (elements_per_thread,))

    # TODO: Support elements_per_thread size mismatch and broadcasting

    assert (
        tuple(insert_type.shape) == vector_shape
    ), f"Shape doesn't match: {tuple(insert_type.shape)} and {(vector_shape)}"

    if not hasattr(node, "index"):
        raise ValidationError("codegen expected write to have index attr.")

    index = node.index

    output_shape = _get_symbolic_shape(memory)
    elements_per_thread = cast_py_literal(emitter, elements_per_thread)
    dyn_vals = tuple(
        cast_vector(emitter, reg, element_type=IndexType.get()) for reg in dyn_vals
    )
    dynamic_vals_map_start = _build_dyn_vals_map(mapping, dyn_vals)
    element_type = kb_ir_type.element_type

    if mapping:
        transformed_index = transform_index_on_mapping(
            mapping, output_shape, index, is_read=False
        )
        mask = _build_mask_with_mapping(
            emitter,
            mapping,
            index,
            transformed_index,
            output_shape,
            elements_per_thread,
            bounds,
            dynamic_vals_map_start,
        )
        index = transformed_index
    else:
        mask = _build_mask(emitter, index, elements_per_thread, bounds)

    start_indices, start_indices_wg, start_indices_th = _build_start_indices(
        emitter, index, dynamic_vals_map_start
    )

    use_llvm_store = flags != MemoryAccessFlags.NONE
    if use_llvm_store:
        _create_llvm_read_write(
            kb_dest, kb_ir_type, start_indices, insert_type, flags, insert_vector
        )
    else:
        _create_vec_read_write(
            emitter,
            output_shape,
            kb_dest,
            insert_vector,
            None,
            start_indices,
            start_indices_wg,
            start_indices_th,
            elements_per_thread,
            get_custom(memory),
            mask,
            node_index=index,
        )


def assume_index_subgroup_uniform(value: Value, element_type: IrType) -> Value:
    res = gpu_d.subgroup_broadcast(value, gpu_d.BroadcastType.first_active_lane)
    return res


def _subs_index_dict(
    index_dict: dict[IndexSymbol, IndexExpr], subs: dict[IndexSymbol, int]
) -> dict[IndexSymbol, IndexExpr]:
    return {k: safe_subs(v, subs) for k, v in index_dict.items()}


@handle_op(tensor_load_to_lds)
def handle_tensor_load_to_lds(emitter: WaveEmitter, node: fx.Node):
    try:
        (
            sources,
            destinations,
            element_type,
            distributed_shape,
            shared_tile_index,
            global_tile_index,
            bounds,
            multicast_mask,
            input_selector,
        ) = node.args
    except ValueError as e:
        raise ValidationError("Malformed arguments") from e

    assert len(sources) == len(
        destinations
    ), "sources and destinations must have the same number of elements."

    i1 = IntegerType.get_signless(1)
    i16 = IntegerType.get_signless(16)
    i32 = IntegerType.get_signless(32)
    v1i16 = VectorType.get([1], i16)
    v16i1 = VectorType.get([16], i1)

    ir_type = IrType.parse(element_type.dtype.ir_type_asm())
    dma_type = amdgpu_d.TDMBaseType.get(ir_type)

    results = []

    subs = add_emitter_subs(emitter)

    for i, (src, dst) in enumerate(zip(sources, destinations)):
        symbolic_shape = _get_symbolic_shape(src)
        # Normalize keys: global_tile_index may have base keys (K) while symbolic_shape has scaled keys (K/2)
        base_to_global_index = {infer_dim(k): v for k, v in global_tile_index.items()}
        global_tile_index_current = {
            k: base_to_global_index[infer_dim(k)] for k in symbolic_shape
        }
        global_tile_index_current = _subs_index_dict(
            global_tile_index_current, {INPUT_SELECTOR: i}
        )

        local_bounds = [
            bounds[s] - global_tile_index_current[s].start for s in symbolic_shape
        ]
        local_bounds = [gen_sympy_index(subs, b) for b in local_bounds]
        local_bounds = [assume_index_subgroup_uniform(b, i32) for b in local_bounds]

        strides = strides_from_symbolic_shape(
            IndexingContext.current(), symbolic_shape, allow_mixed_shapes=True
        )

        distributed_shape_vals = [
            gen_sympy_index(subs, distributed_shape[s]) for s in symbolic_shape
        ]
        distributed_shape_vals = [
            assume_index_subgroup_uniform(v, i32) for v in distributed_shape_vals
        ]

        global_mem = cast_py_value(emitter, src)
        shared_mem = cast_py_value(emitter, dst)

        global_value = global_mem.ir_value
        shared_value = shared_mem.ir_value

        index, _, _ = _build_start_indices(emitter, global_tile_index_current)

        shared_tile_index_current = {k: shared_tile_index[k] for k in symbolic_shape}
        shared_tile_index_current = _subs_index_dict(
            shared_tile_index_current, {INPUT_SELECTOR: i}
        )

        # Calculate shared memory offset from tile indices
        shared_index, _, _ = _build_start_indices(emitter, shared_tile_index_current)
        shared_index = [assume_index_subgroup_uniform(idx, i32) for idx in shared_index]

        base = amdgpu_d.make_dma_base(
            base=dma_type,
            global_=global_value,
            global_indices=index,
            lds=shared_value,
            lds_indices=shared_index,
        )

        pad_interval = None
        pad_amount = None
        original_dst = propagate_loop_carried_vars(dst)
        original_dst = get_custom(original_dst)
        if padding := original_dst.padding:
            bytewidth = element_type.bitwidth() // 8
            unpadded_dim = int(subs_idxc(original_dst.unpadded_shape[-1])) * bytewidth
            assert (
                unpadded_dim >= 8
            ), f"Invalid unpadded_dim for padding: {unpadded_dim} (must be at least 8 bytes)"
            DWORD_SIZE = 4
            pad_interval = arith_d.constant(i32, unpadded_dim // DWORD_SIZE)
            pad_amount = arith_d.constant(i32, (padding * bytewidth) // DWORD_SIZE)

        workgroup_mask = None
        if local_multicast_mask := subs_idxc(
            safe_subs(multicast_mask, {INPUT_SELECTOR: i})
        ):
            local_multicast_mask = sympy.simplify(local_multicast_mask)
            local_multicast_mask_val = gen_sympy_index(subs, local_multicast_mask)
            workgroup_mask = arith_d.index_cast(i16, local_multicast_mask_val)
            workgroup_mask = vector_d.from_elements(v1i16, [workgroup_mask])
            workgroup_mask = vector_d.bitcast(v16i1, workgroup_mask)

        desc = amdgpu_d.make_dma_descriptor(
            base=base,
            global_dynamic_sizes=local_bounds,
            global_static_sizes=[ShapedType.get_dynamic_size()] * len(local_bounds),
            global_dynamic_strides=None,
            global_static_strides=strides,
            shared_dynamic_sizes=distributed_shape_vals,
            shared_static_sizes=[ShapedType.get_dynamic_size()]
            * len(distributed_shape_vals),
            atomic_barrier_indices=None,
            workgroup_mask=workgroup_mask,
            pad_amount=pad_amount,
            pad_interval=pad_interval,
        )

        results.append(desc)

    # Select the appropriate descriptors based on input_selector
    # Build chained select operations for each descriptor
    def select_descriptor(results_list, input_selector_val):
        """Select from list of results using chained arith_d.select operations."""
        assert len(results_list) > 0, "results_list must not be empty"
        if len(results_list) == 1:
            return results_list[0]

        # Start with the last element as default
        selected = results_list[-1]

        # Chain selects from second-to-last backwards to first
        for i in range(len(results_list) - 2, -1, -1):
            # Create condition: selector_val == i
            i_const = arith_d.constant(input_selector_val.type, i)
            cond = arith_d.cmpi(arith_d.CmpIPredicate.eq, input_selector_val, i_const)
            selected = arith_d.select(cond, results_list[i], selected)

        return selected

    input_selector_val = gen_sympy_index(subs, input_selector)
    selected = select_descriptor(results, input_selector_val)

    return amdgpu_d.tensor_load_to_lds(selected)


@handle_op(gather_to_lds)
def handle_gather_to_lds(emitter: WaveEmitter, node: fx.Node):
    try:
        (
            src,
            dst,
            src_idx,
            dst_idx,
            element_type,
            elements_per_thread,
            src_mapping,
            dst_mapping,
            src_bounds,
            src_mapping_dyn_vals,
            dst_mapping_dyn_vals,
        ) = node.args
    except ValueError as e:
        raise ValidationError("Malformed arguments") from e

    element_type = IrType.parse(element_type.dtype.ir_type_asm())

    src_symbolic_shape = _get_symbolic_shape(src)
    dst_symbolic_shape = _get_symbolic_shape(dst)

    src = cast_py_value(emitter, src)
    dst = cast_py_value(emitter, dst)
    src_data_type = get_type_or_element_type(src.ir_value.type)
    dst_data_type = get_type_or_element_type(dst.ir_value.type)

    if not (
        isinstance(src.ir_value.type, MemRefType)
        and isinstance(dst.ir_value.type, MemRefType)
    ):
        op = get_custom(node)
        raise ValidationError(
            f"Expected src and dst to be of Memref type for\n"
            f"{op}\nGot\n"
            f"src: {src.ir_value.type}\n"
            f"dst: {dst.ir_value.type}\n"
        )

    if src_data_type != dst_data_type:
        op = get_custom(node)
        raise ValidationError(
            f"Expected src and dst to have same data type for\n"
            f"{op}\nGot\n"
            f"src: {src_data_type} vs dst: {dst_data_type}\n"
        )

    src = src.ir_value
    dst = dst.ir_value
    src_dynamic_vals_map_start = {}
    dst_dynamic_vals_map_start = {}

    if src_mapping:
        dyn_vals = tuple(
            cast_vector(emitter, reg, element_type=IndexType.get())
            for reg in src_mapping_dyn_vals
        )
        new_src_idx = transform_index_on_mapping(
            src_mapping, src_symbolic_shape, src_idx, is_read=True
        )
        src_dynamic_vals_map_start = _build_dyn_vals_map(src_mapping, dyn_vals)
    else:
        new_src_idx = src_idx
    if dst_mapping:
        dyn_vals = tuple(
            cast_vector(emitter, reg, element_type=IndexType.get())
            for reg in dst_mapping_dyn_vals
        )
        dst_idx = transform_index_on_mapping(
            dst_mapping, dst_symbolic_shape, dst_idx, is_read=False
        )
        dst_dynamic_vals_map_start = _build_dyn_vals_map(dst_mapping, dyn_vals)

    store_type = VectorType.get((elements_per_thread,), element_type)

    src_index, src_index_wg, src_index_th = _build_start_indices(
        emitter, new_src_idx, src_dynamic_vals_map_start
    )

    ip = InsertionPoint.current

    induction_vars = set(emitter.get_induction_vars_and_syms()[1])

    # Hoist to the function level, if not using induction variables.
    if not any(
        induction_vars.intersection(set(index.start.free_symbols))
        for index in dst_idx.values()
    ):
        while not isinstance(ip.block.owner, func_d.FuncOp):
            ip = InsertionPoint(ip.block.owner)

    with ip:
        dst_index, _, _ = _build_start_indices(
            emitter, dst_idx, dst_dynamic_vals_map_start
        )
        # We are indexing shared mem so i32 is enough.
        i32 = IntegerType.get_signless(32)
        dst_index = [assume_index_subgroup_uniform(idx, i32) for idx in dst_index]

    strides = strides_from_symbolic_shape(
        IndexingContext.current(), src_symbolic_shape, allow_mixed_shapes=True
    )
    strides = [
        gen_sympy_index(add_emitter_subs(emitter, src_dynamic_vals_map_start), s)
        for s in strides
    ]

    src, offset_th = _linearize_memref(src, src_index_wg, src_index_th, strides)
    src = _cast_buffer_and_encode_stride(src, strides, element_type, emitter)

    # We previously checked mask is same for all elements, so we can use
    # elements_per_thread=1 to build the mask.
    mask = _build_mask(
        emitter,
        src_idx,
        elements_per_thread=1,
        bounds=src_bounds,
        dynamic_values=src_dynamic_vals_map_start,
    )
    if mask:
        mask = vector_d.extract(mask, static_position=[0], dynamic_position=[])
        oob_index_value = _get_out_of_bounds_index(element_type)
        oob_index = arith_d.constant(IndexType.get(), oob_index_value)
        offset_th = arith_d.select(mask, offset_th, oob_index)

    src_index = [offset_th]

    amdgpu_d.gather_to_lds(
        src=src,
        src_indices=src_index,
        dst=dst,
        dst_indices=dst_index,
        transfer_type=store_type,
    )


def _handle_scatter_op(
    emitter: WaveEmitter,
    node: fx.Node,
    rmw_kind: arith_d.AtomicRMWKind,
):
    try:
        (
            register_src,
            register_idx,
            dim,
            memory,
            mapping,
            elements_per_thread,
            bounds,
        ) = node.args
    except ValueError as e:
        raise ValidationError("Malformed arguments") from e

    output_shape = _get_symbolic_shape(memory)
    elements_per_thread = int(cast_py_literal(emitter, elements_per_thread))
    cast_vector(emitter, register_idx, element_type=IndexType.get())

    index_mapping = mapping.map_output_indices(output_shape)

    idxc = IndexingContext.current()
    index_mapping = tuple(i.subs(idxc.subs) for i in index_mapping)
    iters = mapping.iters
    index = node.index
    subs = [
        (sym, expr.start) for sym, expr in zip(iters.keys(), index.values())
    ] + list(idxc.subs.items())

    result_index = {key: m.subs(subs) for key, m in zip(output_shape, index_mapping)}

    mask = _build_mask(emitter, index, elements_per_thread, bounds)
    if mask is None:
        mask_vec_type = VectorType.get(
            [elements_per_thread], IntegerType.get_signless(1)
        )
        mask = _constant_mask(mask_vec_type)

    start_indices, start_indices_wg, start_indices_th = _build_start_indices(
        emitter, result_index
    )

    register_idx = cast_py_value(emitter, register_idx).ir_value
    register_src = cast_py_value(emitter, register_src).ir_value
    memory = cast_py_value(emitter, memory).ir_value

    results = []
    for i in range(elements_per_thread):
        index_elem = vector_d.extract(
            register_idx, static_position=[i], dynamic_position=[]
        )
        index_elem = arith_d.index_cast(IndexType.get(), index_elem)
        reg_elem = vector_d.extract(
            register_src, static_position=[i], dynamic_position=[]
        )
        indices = list(start_indices)
        if dim >= len(indices):
            raise ValueError(
                f"Invalid scatter dim {dim} for rank-{len(indices)} memory"
            )

        indices[dim] = index_elem

        # In case 4 elements per thread are used, makes sure values are stored at the right non-scatter dimension
        if elements_per_thread > 1:
            other_dims = [d for d in range(len(indices)) if d != dim]
            if other_dims:
                # Heuristic: offset the innermost (fastest varying) dimension
                # TODO: Ideally emit a vectorized atomic op instead of 4 scalar atomics that store to consecutive locations
                fast_dim = other_dims[-1]
                indices[fast_dim] = arith_d.addi(
                    indices[fast_dim], arith_d.constant(IndexType.get(), i)
                )
        result = memref_d.atomic_rmw(rmw_kind, reg_elem, memory, indices)
        results.append(result)

    result_type = VectorType.get([elements_per_thread], register_src.type.element_type)
    result_vector = vector_d.from_elements(result_type, results)


@handle_op(scatter_add)
def handle_scatter_add(emitter: WaveEmitter, node: fx.Node):
    register_src = cast_py_value(emitter, node.args[0])
    src_data_type = get_type_or_element_type(register_src.ir_value.type)
    if is_float_type(src_data_type):
        rmw_kind = arith_d.AtomicRMWKind.addf
    else:
        rmw_kind = arith_d.AtomicRMWKind.addi
    _handle_scatter_op(emitter, node, rmw_kind)
