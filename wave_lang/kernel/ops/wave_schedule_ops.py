from dataclasses import dataclass
from typing import Any, Sequence, TYPE_CHECKING
import torch.fx as fx
from ..ops.wave_ops import (
    get_custom,
    Read,
    Write,
    SharedMemoryBarrier,
    WorkgroupBarrier,
    SchedulingBarrier,
)
from ..wave.constraints import Constraint
from .base import define_schedule_op
import logging

if TYPE_CHECKING:
    from .._support.tracing import CapturedTrace

logger = logging.getLogger(__name__)


def get_proxy_result(proxy):
    """
    Get the real result for a proxy from the current schedule context.

    Args:
        proxy: The proxy to resolve

    Returns:
        The real result if found in the context, otherwise None
    """
    from .._support.tracing import ScheduleContext

    current_context = ScheduleContext.current()
    if current_context is not None:
        return current_context.get_proxy_result(proxy)
    return None


def create_schedule_proxy(
    region_graph,
    real_value: Any,
    op_name: str = "schedule_op",
):
    """
    Create a proxy for a schedule operation that embeds the real value.

    Args:
        region_graph: The region graph to create the proxy in
        real_value: The real value to embed in the proxy
        op_name: The name of the operation for debugging

    Returns:
        The created proxy
    """

    # Create a proxy function that returns the embedded real value
    def proxy_func(*proxy_args, **proxy_kwargs):
        return real_value

    # Set the function name for better debugging
    proxy_func.__name__ = op_name

    try:
        proxy = region_graph.create_proxy(
            "call_function",
            proxy_func,
            (),
            {},
        )

        from .._support.tracing import ScheduleContext

        current_context = ScheduleContext.current()
        if current_context is not None:
            current_context.proxy_to_results[proxy] = real_value

        return proxy

    except Exception as e:
        logger.exception("Failed to create schedule proxy for op '%s': %s", op_name, e)
        raise ValueError(
            f"Failed to create schedule proxy for op '{op_name}': {e}"
        ) from e


def empty_proxy(name: str = "empty_proxy"):
    from .._support.tracing import ScheduleContext

    current_context = ScheduleContext.current()
    if current_context is not None:
        return create_schedule_proxy(
            current_context.region_graph,
            None,
            name,
        )


# Stubs to enable type checking of the custom schedule ops - decorated with @define_op for dispatch
@define_schedule_op
def get_node_by_tag(tag: str): ...


@define_schedule_op
def get_node_by_tag_and_type(tag: str, node_type: Any): ...


@define_schedule_op
def get_nodes_in_subgraph(loop: Any, tag: str = None, node_type: Any = None): ...


@define_schedule_op
def partition_by_address_space(node: Any, address_space: Any): ...


@define_schedule_op
def partition_by_dim(nodes: Any, dim: Any, factor: int): ...


@define_schedule_op
def insert_before(nodes: Any, op: Any): ...


@define_schedule_op
def insert_after(nodes: Any, op: Any): ...


@define_schedule_op
def reorder_graph(loop: Any, clusters: Any): ...


@define_schedule_op
def pipeline(iterate: Sequence[fx.Node]): ...


@define_schedule_op
def getitem(obj: Any, index: int): ...


def get_node_by_tag_helper(kernel_trace, tag: str):
    logger.info(f"Getting node by tag: {tag}")
    nodes = kernel_trace.walk(lambda x: get_custom(x).tag == tag)
    logger.info(f"Found {len(nodes)} nodes by tag: {tag}")
    return nodes


@dataclass
class CustomScheduleOp:
    """Base class for custom schedule operations."""

    pass


@dataclass
class GetNodeByTag(CustomScheduleOp):
    tag: str
    schedule_op_name = "get_node_by_tag"

    @classmethod
    def handle(
        cls, region_graph, kernel_trace, constraints: list[Constraint], tag: str
    ):
        # Always execute the real logic during tracing to apply scheduling
        real_result = get_node_by_tag_helper(kernel_trace, tag)

        # Create a proxy that embeds the real result
        return create_schedule_proxy(
            region_graph,
            real_result,
            cls.schedule_op_name,
        )


@dataclass
class GetNodesInSubgraph(CustomScheduleOp):
    loop: Any
    tag: str = None
    node_type: Any = None
    schedule_op_name = "get_nodes_in_subgraph"

    @classmethod
    def handle(
        cls,
        region_graph,
        kernel_trace,
        constraints: list[Constraint],
        loop: Any,
        tag: str = None,
        node_type: Any = None,
    ):
        # Get the iterate node from the proxy
        assert hasattr(
            loop, "node"
        ), f"Expected 'loop' to be a proxy object with a 'node' attribute"
        loop_result = get_proxy_result(loop.node)
        assert loop_result is not None, "Loop must have a result"
        assert len(loop_result) > 0, "Loop must have at least one element"

        iterate_node = loop_result[0]
        custom_iterate = get_custom(iterate_node)
        subgraph = kernel_trace.get_subgraph(custom_iterate.subgraph_name)

        # Filter nodes in the subgraph
        if tag and node_type:
            nodes = [
                node
                for node in subgraph.nodes
                if hasattr(get_custom(node), "tag")
                and get_custom(node).tag == tag
                and isinstance(get_custom(node), node_type)
            ]
        elif tag:
            nodes = [
                node
                for node in subgraph.nodes
                if hasattr(get_custom(node), "tag") and get_custom(node).tag == tag
            ]
        elif node_type:
            nodes = [
                node
                for node in subgraph.nodes
                if isinstance(get_custom(node), node_type)
            ]
        else:
            nodes = list(subgraph.nodes)

        logger.info(
            f"Found {len(nodes)} nodes in subgraph '{custom_iterate.subgraph_name}' with tag='{tag}', type={node_type}"
        )

        return create_schedule_proxy(
            region_graph,
            nodes,
            cls.schedule_op_name,
        )


@dataclass
class GetNodeByTagAndType(CustomScheduleOp):
    tag: str
    node_type: Any
    schedule_op_name = "get_node_by_tag_and_type"

    @classmethod
    def handle(
        cls,
        region_graph,
        kernel_trace,
        constraints: list[Constraint],
        tag: str,
        node_type: Any,
    ):
        assert constraints is not None, "Constraints are required"

        nodes = get_node_by_tag_helper(kernel_trace, tag)
        nodes = [node for node in nodes if isinstance(get_custom(node), node_type)]
        logger.info(f"Found {len(nodes)} nodes by tag: {tag} and type: {node_type}")

        return create_schedule_proxy(
            region_graph,
            nodes,
            cls.schedule_op_name,
        )


@dataclass
class PartitionByAddressSpace(CustomScheduleOp):
    node: Any
    address_space: Any
    schedule_op_name = "partition_by_address_space"

    @classmethod
    def handle(
        cls,
        region_graph,
        kernel_trace,
        constraints: list[Constraint],
        nodes: Sequence[fx.Node],
        address_space: Any,
    ):
        matched, unmatched = [], []

        assert hasattr(
            nodes, "node"
        ), f"Expected 'nodes' to be a proxy object with a 'node' attribute, but got type: {type(nodes).__name__}"
        nodes = get_proxy_result(nodes.node)
        assert nodes is not None, "Nodes must have a result"
        assert len(nodes) > 0, "Nodes must have at least one element"

        assert all(
            [
                isinstance(get_custom(node), Read)
                or isinstance(get_custom(node), Write)
                for node in nodes
            ]
        ), "Nodes to partition must be Read or Write"
        matched = [
            node
            for node in nodes
            if get_custom(node).memory_type.address_space == address_space
        ]
        unmatched = [node for node in nodes if node not in matched]

        return create_schedule_proxy(
            region_graph,
            (matched, unmatched),
            cls.schedule_op_name,
        )


@dataclass
class InsertBefore(CustomScheduleOp):
    nodes: Any
    op: Any
    schedule_op_name = "insert_before"

    @classmethod
    def handle(
        cls,
        region_graph,
        kernel_trace,
        constraints: list[Constraint],
        nodes: Any,
        op: Any,
    ):
        # Get the actual nodes from the proxy
        assert hasattr(
            nodes, "node"
        ), f"Expected 'nodes' to be a proxy object with a 'node' attribute, but got type: {type(nodes).__name__}"
        nodes_list = get_proxy_result(nodes.node)
        assert nodes_list is not None, "Nodes must have a result"

        if not isinstance(nodes_list, (list, tuple)):
            nodes_list = [nodes_list]
        assert len(nodes_list) > 0, "Nodes must have at least one element"

        # Get the subgraph that these nodes belong to
        first_node = nodes_list[0]
        subgraph = first_node.graph

        # Get location from first node for context
        custom = get_custom(first_node)
        context_location = custom.location if hasattr(custom, "location") else None

        # Add the operation to the subgraph
        # op should already be an initialized CustomOp instance
        new_op = op.add_to_graph(subgraph)
        if context_location:
            new_op.location = context_location

        # Return list with new op prepended
        result_nodes = [new_op] + list(nodes_list)

        return create_schedule_proxy(
            region_graph,
            result_nodes,
            cls.schedule_op_name,
        )


@dataclass
class InsertAfter(CustomScheduleOp):
    nodes: Any
    op: Any
    schedule_op_name = "insert_after"

    @classmethod
    def handle(
        cls,
        region_graph,
        kernel_trace,
        constraints: list[Constraint],
        nodes: Any,
        op: Any,
    ):
        # Get the actual nodes from the proxy
        assert hasattr(
            nodes, "node"
        ), f"Expected 'nodes' to be a proxy object with a 'node' attribute, but got type: {type(nodes).__name__}"
        nodes_list = get_proxy_result(nodes.node)
        assert nodes_list is not None, "Nodes must have a result"

        if not isinstance(nodes_list, (list, tuple)):
            nodes_list = [nodes_list]
        assert len(nodes_list) > 0, "Nodes must have at least one element"

        # Get the subgraph that these nodes belong to
        last_node = nodes_list[-1]
        subgraph = last_node.graph

        # Get location from last node for context
        custom = get_custom(last_node)
        context_location = custom.location if hasattr(custom, "location") else None

        # Add the operation to the subgraph
        # op should already be an initialized CustomOp instance
        new_op = op.add_to_graph(subgraph)
        if context_location:
            new_op.location = context_location

        # Return list with new op appended
        result_nodes = list(nodes_list) + [new_op]

        return create_schedule_proxy(
            region_graph,
            result_nodes,
            cls.schedule_op_name,
        )


@dataclass
class ReorderGraph(CustomScheduleOp):
    loop: Any
    clusters: Any
    schedule_op_name = "reorder_graph"

    @classmethod
    def handle(
        cls,
        region_graph,
        kernel_trace,
        constraints: list[Constraint],
        loop: Any,
        clusters: Any,
    ):
        from ..wave.schedule_reordering import reorder_graph as reorder_graph_impl

        breakpoint()
        # Get the iterate node from the proxy
        assert hasattr(
            loop, "node"
        ), f"Expected 'loop' to be a proxy object with a 'node' attribute, but got type: {type(loop).__name__}"
        loop_result = get_proxy_result(loop.node)
        assert loop_result is not None, "Loop must have a result"
        assert len(loop_result) > 0, "Loop must have at least one element"

        # Get the iterate's subgraph (this will be the KERNEL stage after pipelining)
        iterate_node = loop_result[0]
        custom_iterate = get_custom(iterate_node)
        subgraph = kernel_trace.get_subgraph(custom_iterate.subgraph_name)

        # Extract the actual cluster nodes from proxies
        # clusters should be a list of nodes/operations
        cluster_nodes = []
        if hasattr(clusters, "node"):
            # If clusters is a single proxy
            clusters_result = get_proxy_result(clusters.node)
            if clusters_result:
                cluster_nodes = clusters_result
        elif isinstance(clusters, (list, tuple)):
            # If clusters is a list of proxies or nodes
            for item in clusters:
                if hasattr(item, "node"):
                    result = get_proxy_result(item.node)
                    if result:
                        cluster_nodes.append(result)
                else:
                    cluster_nodes.append(item)
        else:
            raise ValueError(f"Unexpected clusters type: {type(clusters)}")

        logger.info(f"Reordering with {len(cluster_nodes)} cluster items")

        # Apply the reordering to the subgraph
        breakpoint()
        reordered_subgraph = reorder_graph_impl(subgraph, cluster_nodes)

        if reordered_subgraph is None:
            logger.warning("Failed to reorder graph, skipping reordering")
            return empty_proxy("reorder_graph_failed")

        # Replace the old subgraph with the reordered one
        reordered_subgraph.parent_op = subgraph.parent_op
        original_subgraph_name = custom_iterate.subgraph_name
        reordered_subgraph_name = f"reordered_{original_subgraph_name}"

        # Add the new subgraph and update references
        kernel_trace.add_subgraph(reordered_subgraph_name, reordered_subgraph)
        kernel_trace.get_root_graph().subgraphs[
            reordered_subgraph_name
        ] = reordered_subgraph
        custom_iterate.update_arg("subgraph_name", reordered_subgraph_name)

        # Remove the old subgraph
        del kernel_trace.region_graph.subgraphs[original_subgraph_name]
        del kernel_trace.get_root_graph().subgraphs[original_subgraph_name]

        logger.info(
            f"Successfully reordered graph: {original_subgraph_name} -> {reordered_subgraph_name}"
        )

        return empty_proxy("reorder_graph_success")


@dataclass
class PartitionByDim(CustomScheduleOp):
    nodes: Any
    dim: Any
    factor: int
    schedule_op_name = "partition_by_dim"

    @classmethod
    def handle(
        cls,
        region_graph,
        kernel_trace,
        constraints: list[Constraint],
        nodes: Any,
        dim: Any,
        factor: int,
    ):
        # Get the actual nodes from the proxy
        assert hasattr(
            nodes, "node"
        ), f"Expected 'nodes' to be a proxy object with a 'node' attribute, but got type: {type(nodes).__name__}"
        nodes_list = get_proxy_result(nodes.node)
        assert nodes_list is not None, "Nodes must have a result"
        assert len(nodes_list) > 0, "Nodes must have at least one element"

        partitioned_nodes = [[] for _ in range(factor)]

        # Get all unique dimension IDs for the specified dimension
        dim_ids = set()
        for node in nodes_list:
            custom = get_custom(node)
            if custom.expanded_dims and dim in custom.expanded_dims:
                dim_ids.add(custom.expanded_dims[dim])

        # Validate that the dimension can be partitioned by the factor
        dim_expand_size = len(dim_ids)
        assert dim_expand_size >= factor and dim_expand_size % factor == 0, (
            f"Dimension {dim} has size {dim_expand_size} which cannot be evenly "
            f"partitioned by factor {factor}"
        )
        assert all(
            x in dim_ids for x in range(dim_expand_size)
        ), f"Dimension {dim} IDs are not contiguous: {sorted(dim_ids)}"

        size_of_partition = dim_expand_size // factor
        for node in nodes_list:
            custom = get_custom(node)
            if custom.expanded_dims and dim in custom.expanded_dims:
                dim_id = custom.expanded_dims[dim]
                partition_id = dim_id // size_of_partition
                partitioned_nodes[partition_id].append(node)
            else:
                # If node doesn't have the dimension, add to all partitions
                # This matches behavior for nodes that aren't expanded along this dim
                for partition in partitioned_nodes:
                    partition.append(node)

        # Return tuple of partitioned node lists
        return create_schedule_proxy(
            region_graph,
            tuple(partitioned_nodes),
            cls.schedule_op_name,
        )


class PipelinedLoop:
    def __init__(
        self,
        iterate: Sequence[fx.Node],
        kernel_trace: "CapturedTrace",
        constraints: list[Constraint],
    ):
        self.iterate = iterate
        self.kernel_trace = kernel_trace
        self.constraints = constraints
        self.initiation_interval = None
        self.num_stages = 0
        self.barrier_insertions = []

    def __enter__(self):
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        from ..wave.scheduling.schedule import (
            propagate_scheduling_parameters_to_iter_args,
            apply_pipelined_schedule,
        )
        from ..wave.scheduling.schedule_enums import SchedulingType

        result = get_proxy_result(self.iterate)
        assert result is not None, "Iterate must have a result"
        assert isinstance(result, Sequence), "Iterate must be a sequence"
        assert len(result) == 1, "Iterate must have exactly one element"

        custom_iterate = get_custom(result[0])
        subgraph = self.kernel_trace.get_subgraph(custom_iterate.subgraph_name)

        propagate_scheduling_parameters_to_iter_args(
            subgraph,
            self.initiation_interval,
        )

        # Apply barrier insertions before constructing the pipelined loop
        self._apply_barrier_insertions(subgraph)

        apply_pipelined_schedule(
            custom_iterate,
            subgraph,
            self.kernel_trace,
            self.constraints,
            use_scheduling_barriers=True,
            num_stages=self.num_stages,
            initiation_interval=self.initiation_interval,
            scheduling_type=SchedulingType.MANUAL,
            visualize=False,
            multi_buffer_count=None,
        )

    def set_stage(self, nodes: Sequence[fx.Node]):
        stage = self.num_stages
        if self.initiation_interval is None:
            self.initiation_interval = len(nodes)
        else:
            assert self.initiation_interval == len(
                nodes
            ), "The number of clusters must be the same across stages"
        result_clusters = []
        for cluster in nodes:
            result_nodes = []
            for node in cluster:
                node_result = get_proxy_result(node)
                assert node_result is not None, "Nodes must have a result"
                result_nodes.append(node_result)
            result_clusters.append(tuple(result_nodes))

        node_order = 0
        for i, cluster in enumerate(result_clusters):
            for nodes in cluster:
                for node in nodes:
                    custom = get_custom(node)
                    custom.scheduling_parameters = {
                        "absolute_cycle": stage * self.initiation_interval + i,
                        "stage": stage,
                        "initiation_interval": self.initiation_interval,
                        "prefetch_stage": None,
                        "order": node_order,
                    }
                    custom.scheduling_parameters["cycle"] = (
                        custom.scheduling_parameters["absolute_cycle"]
                        % self.initiation_interval
                    )
                    node_order += 1

        self.num_stages += 1
        # During tracing, return a proxy for the set_stage operation
        return empty_proxy("set_stage")

    def insert_barrier_after(self, proxy, barrier_type="shared_memory"):
        """
        Insert a barrier after the operations in proxy.
        """

        # assert that barrier type is one of "shared_memory" or "workgroup" or "scheduling"
        assert barrier_type in [
            "shared_memory",
            "workgroup",
            "scheduling",
        ], "Invalid barrier type"
        self.barrier_insertions.append(
            {
                "proxy": proxy,
                "barrier_type": barrier_type,
                "position": "after",
            }
        )
        return self

    def _apply_barrier_insertions(self, subgraph: fx.Graph):
        """
        Apply all queued barrier insertions to the subgraph.
        """

        for insertion in self.barrier_insertions:
            # Get the actual nodes from the proxy
            target_nodes = get_proxy_result(insertion["proxy"])
            if not target_nodes:
                continue

            if not isinstance(target_nodes, (list, tuple)):
                target_nodes = [target_nodes]

            # Find the last node in the list (to insert after)
            last_node = target_nodes[-1]

            # Get scheduling parameters from the last target node
            custom = get_custom(last_node)
            if (
                not hasattr(custom, "scheduling_parameters")
                or custom.scheduling_parameters is None
            ):
                print(
                    f"Warning: Node {last_node} has no scheduling parameters, skipping barrier insertion"
                )
                continue

            # Insert barrier after the last target node in the graph
            barrier = None
            with subgraph.inserting_after(last_node):
                if insertion["barrier_type"] == "scheduling":
                    barrier = SchedulingBarrier([]).add_to_graph(
                        subgraph, loc=custom.location
                    )
                elif insertion["barrier_type"] == "workgroup":
                    barrier = WorkgroupBarrier().add_to_graph(
                        subgraph, loc=custom.location
                    )
                else:
                    barrier = SharedMemoryBarrier().add_to_graph(
                        subgraph, loc=custom.location
                    )

            # Copy scheduling parameters from the target node (for the rest of the passes)
            barrier_custom = get_custom(barrier)
            barrier_custom.scheduling_parameters = custom.scheduling_parameters.copy()


@dataclass
class GetItem(CustomScheduleOp):
    obj: Any
    index: int
    schedule_op_name = "getitem"

    @classmethod
    def handle(
        cls,
        region_graph,
        kernel_trace,
        constraints: list[Constraint],
        obj: Any,
        index: int,
    ):
        if hasattr(obj, "node"):
            source_result = get_proxy_result(obj.node)
            if isinstance(source_result, (list, tuple)) and len(source_result) > index:
                real_result = source_result[index]
            else:
                raise ValueError(f"Index {index} out of bounds for {source_result}")
        else:
            raise ValueError(f"Object {obj} is not a proxy")

        return create_schedule_proxy(
            region_graph,
            real_result,
            cls.schedule_op_name,
        )


@dataclass
class Pipeline(CustomScheduleOp):
    schedule_op_name = "pipeline"

    @classmethod
    def handle(
        cls,
        region_graph,
        kernel_trace,
        constraints: list[Constraint],
        iterate: Sequence[fx.Node],
    ):
        real_pipelined_loop = PipelinedLoop(iterate, kernel_trace, constraints)

        create_schedule_proxy(
            region_graph,
            real_pipelined_loop,
            cls.schedule_op_name,
        )

        # For context manager support, return the real object
        return real_pipelined_loop
